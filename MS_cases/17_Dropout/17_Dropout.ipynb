{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2509b842",
   "metadata": {},
   "source": [
    "# 17  基于MindSpore构造Dropout层\n",
    "\n",
    "本实验主要介绍使用MindSpore深度学习框架中使用Dropout层，并对比Dropout层对深度学习模型性能的影响"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8175d5a",
   "metadata": {},
   "source": [
    "# 1 实验目的\n",
    "- 掌握如何使用Mindspore深度学习框架构造Dropout层；\n",
    "- 了解Dropout层的相关原理以及应用；\n",
    "- 了解dropout层对于深度学习模型性能的影响;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbddd775",
   "metadata": {},
   "source": [
    "# 2 Dropout层背景及知识点介绍\n",
    "## 2.1背景知识\n",
    "2012年，Hinton在论文[《Improving neural networks by preventing co-adaptation of feature detectors》](https://arxiv.org/pdf/1207.0580.pdf)中提出Dropout。当一个复杂的前馈神经网络被训练在小数据集时，容易造成过拟合。为了防止过拟合，可以通过阻止特征检测器的共同作用来提高神经网络的性能。同年，Alex、Hinton在论文[《ImageNet Classification with Deep Convolutional Neural Networks》](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=1166C42C86BD3A3C6C75B53E2CD2E14F?doi=10.1.1.299.205&rep=rep1&type=pdf)中使用了Dropout算法，用于防止过拟合。该论文赢得了2012年图像识别大赛冠军。随后，关于Dropout的论文[《Dropout:A Simple Way to Prevent Neural Networks from Overfitting》](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)、[《Improving Neural Networks with Dropout》](http://www.cs.toronto.edu/~nitish/msc_thesis.pdf)、[《Dropout as data augmentation》](https://arxiv.org/pdf/1506.08700.pdf)等不断提高Dropout层的应用。\n",
    "\n",
    "## 2.2Dropout原理介绍\n",
    "\n",
    "Dropout作为训练深度神经网络的trick供开发者选择。其原理是在每个训练批次中，通过忽略一半的特征检测器（让一半的隐层节点值为0），从而地减少过拟合现象。这种方式可以减少特征检测器（隐层节点）间的相互作用（检测器相互作用是指某些检测器依赖其他检测器才能发挥作用）。具体的说：在前向传播时，让某个神经元的激活值以一定的概率p停止工作，从而使模型泛化性更强，减少对某些局部的特征的依赖。\n",
    "\n",
    "![avatar](fig/fig1.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd5823dd",
   "metadata": {},
   "source": [
    "# 3 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.0；Python环境=3.7。\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e191b6",
   "metadata": {},
   "source": [
    "# 4. 数据处理\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "809458b1",
   "metadata": {},
   "source": [
    "## 4.1 数据准备"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30144e4c",
   "metadata": {},
   "source": [
    "MNIST数据集是机器学习领域中经典的数据集，由60,000个训练样本和10,000个测试样本组成，每张图片都是28x28像素大小的灰度图像，数字从0到9。\n",
    "\n",
    "MNIST数据集主要用于图像识别领域的研究，特别是用于机器学习算法的测试和比较。由于它的简单性和易于使用，MNIST已经成为机器学习社区中的标准数据集之一。\n",
    "\n",
    "以下是几个示例图像：\n",
    "\n",
    "![jupyter](./fig/Fig001.png)\n",
    "\n",
    "每个图像都被转换成784个数字的一维向量（28×28=784）。这些数字表示像素的灰度值，值的范围从0（黑色）到255（白色）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50e7ef77",
   "metadata": {},
   "source": [
    "从开放数据集中下载MNIST数据集的压缩包,并解压储存在项目的根目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1162ceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip (10.3 MB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 10.8M/10.8M [00:01<00:00, 8.86MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    }
   ],
   "source": [
    "# 从开放数据集中下载MNIST数据集\n",
    "from download import download\n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, \"./\", kind=\"zip\", replace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7e57aec",
   "metadata": {},
   "source": [
    "下载后的数据集目录结构如下：\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84c2fa8f",
   "metadata": {},
   "source": [
    "/MNIST_Data\\\n",
    "├── train\\\n",
    "&emsp;├──train-images-idx3-ubyte\\\n",
    "&emsp;└──train-labels-idx1-ubyte\\\n",
    "├──text\\\n",
    "&emsp;├──t10k-images-idx3-ubyte\\\n",
    "&emsp;└──t10k-labels-idx1-ubyte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0abc5cb",
   "metadata": {},
   "source": [
    "## 4.2 设置超参数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a1f6ed1",
   "metadata": {},
   "source": [
    "集中设置在模型训练过程中需要使用到的超参数的具体数值，方便在后续调试代码时修改超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95181c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE= 64       # batch的大小\n",
    "LEARNING_RATE = 1e-2 # 学习率\n",
    "EPOCH = 3      # 迭代次数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0497d5",
   "metadata": {},
   "source": [
    "## 4.3 获取数据集对象\n",
    "利用mindspore中的dataset相关的函数读取数据集中的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1cc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.dataset import transforms\n",
    "# 读取和解析Manifest数据文件构建数据集\n",
    "from mindspore.dataset import MnistDataset\n",
    "train_dataset = MnistDataset('MNIST_Data/train')\n",
    "test_dataset = MnistDataset('MNIST_Data/test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "190d28a0",
   "metadata": {},
   "source": [
    "## 4.4 数据处理 \n",
    "MindSpore的dataset使用数据处理流水线（Data Processing Pipeline），需指定map、batch、shuffle等操作。这里我们使用map对图像数据及标签进行变换处理，然后将处理好的数据集打包为大小为64的batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c12fc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MindSpore库\n",
    "import mindspore\n",
    "# 神经网络模块\n",
    "from mindspore import nn\n",
    "# 常见算子操作\n",
    "from mindspore import ops\n",
    "# 图像增强模块\n",
    "from mindspore.dataset import vision\n",
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        # 基于给定的缩放和平移因子调整图像的像素大小。输出图像的像素大小为：output = image * rescale + shift。\n",
    "        # 此处rescale取1.0 / 255.0，shift取0\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        # 正则化 均值为0.1307，标准差为0.3081（查自官网）\n",
    "        vision.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "        # 将输入图像的shape从 <H, W, C> 转换为 <C, H, W>\n",
    "        vision.HWC2CHW()\n",
    "    ]\n",
    "    # 将输入的Tensor转换为指定的数据类型。\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    # map给定一组数据增强列表，按顺序将数据增强作用在数据集对象上。\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    # 将数据集中连续 batch_size 条数据组合为一个批数据\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "# 对数据集进行transfrom和batch\n",
    "train_dataset = datapipe(train_dataset, BATCH_SIZE)\n",
    "test_dataset = datapipe(test_dataset, BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "227f311a",
   "metadata": {},
   "source": [
    "# 5. 实验过程\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "030dceb8",
   "metadata": {},
   "source": [
    "## 5.1 导入所需库和函数\n",
    "在代码最开始集中导入整个实验中所需要使用的库和函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c481114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通用数据增强\n",
    "from mindspore.dataset import transforms\n",
    "# 读取和解析Manifest数据文件构建数据集\n",
    "from mindspore.dataset import MnistDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0d4d5eb",
   "metadata": {},
   "source": [
    "## 5.2 模型构建\n",
    "使用mindspore中提供的相关函数构建训练使用的神经网络模型，包括平图层，致密连接层等，并在construct中完成神经网络的前向构造。\\\n",
    "为了对比dropout层对深度学习的影响，做如下设置：\n",
    "1. 构建两个除了dropout层完全相同的网络模型。\n",
    "2. 采用完全相同的超参数及相关设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd8360b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): Dropout<p=0.7>\n",
      "    (4): ReLU<>\n",
      "    (5): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 定义附加dropout层的模型\n",
    "# MindSpore 中提供用户通过继承 nn.Cell 来方便用户创建和执行自己的网络\n",
    "class Network(nn.Cell): \n",
    "    # 自定义的网络中，需要在__init__构造函数中申明各个层的定义\n",
    "    def __init__(self): \n",
    "         # 继承父类nn.cell的__init__方法\n",
    "        super().__init__()         \n",
    "        # nn.Flatten为输入展成平图层，即去掉那些空的维度\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            # nn.Dense为致密连接层，它的第一个参数为输入层的维度，第二个参数为输出的维度，\n",
    "            # 第三个参数为神经网络可训练参数W权重矩阵的初始化方式，默认为normal\n",
    "            # nn.ReLU()非线性激活函数，它往往比论文中的sigmoid激活函数具有更好的效益\n",
    "            nn.Dense(28 * 28, 512), # 致密连接层 输入28*28 输出512\n",
    "            nn.ReLU(),              # ReLU层\n",
    "            nn.Dense(512, 512),     # 致密连接层 输入512 输出512\n",
    "            nn.Dropout(p=0.7),# Dropout层，舍弃率为0.7\n",
    "            nn.ReLU(),              # ReLu层\n",
    "            # nn.Dropout(keep_prob=0.1),# Dropout层，舍弃率为0.1\n",
    "            nn.Dense(512, 10)       # 致密连接层 输入512 输出10\n",
    "            # nn.Dropout(keep_prob=0.1)# Dropout层，舍弃率为0.1\n",
    "        \n",
    "        )\n",
    "    # 在construct中实现层之间的连接关系，完成神经网络的前向构造\n",
    "    def construct(self, x):\n",
    "         #调用init中定义的self.flatten()方法\n",
    "        x = self.flatten(x)\n",
    "        #调用init中的self.dense_relu_sequential()方法\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        # 返回模型\n",
    "        return logits\n",
    "model = Network()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c668f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network2<\n",
      "  (flatten): Flatten<>\n",
      "  (dense_relu_sequential): SequentialCell<\n",
      "    (0): Dense<input_channels=784, output_channels=512, has_bias=True>\n",
      "    (1): ReLU<>\n",
      "    (2): Dense<input_channels=512, output_channels=512, has_bias=True>\n",
      "    (3): ReLU<>\n",
      "    (4): Dense<input_channels=512, output_channels=10, has_bias=True>\n",
      "    >\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "# 定义不附加dropout层的模型\n",
    "class Network2(nn.Cell): \n",
    "    # 自定义的网络中，需要在__init__构造函数中申明各个层的定义\n",
    "    def __init__(self): \n",
    "         # 继承父类nn.cell的__init__方法\n",
    "        super().__init__()         \n",
    "        # nn.Flatten为输入展成平图层，即去掉那些空的维度\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 使用SequentialCell对网络进行管理\n",
    "        self.dense_relu_sequential = nn.SequentialCell(\n",
    "            # nn.Dense为致密连接层，它的第一个参数为输入层的维度，第二个参数为输出的维度，\n",
    "            # 第三个参数为神经网络可训练参数W权重矩阵的初始化方式，默认为normal\n",
    "            # nn.ReLU()非线性激活函数，它往往比论文中的sigmoid激活函数具有更好的效益\n",
    "            nn.Dense(28 * 28, 512), # 致密连接层 输入28*28 输出512\n",
    "            # nn.Dropout(keep_prob=0.3),# Dropout层，舍弃率为0.3\n",
    "            nn.ReLU(),              # ReLU层\n",
    "            nn.Dense(512, 512),     # 致密连接层 输入512 输出512\n",
    "            nn.ReLU(),              # ReLu层\n",
    "            nn.Dense(512, 10)       # 致密连接层 输入512 输出10\n",
    "        \n",
    "        )\n",
    "    # 在construct中实现层之间的连接关系，完成神经网络的前向构造\n",
    "    def construct(self, x):\n",
    "         #调用init中定义的self.flatten()方法\n",
    "        x = self.flatten(x)\n",
    "        #调用init中的self.dense_relu_sequential()方法\n",
    "        logits = self.dense_relu_sequential(x)\n",
    "        # 返回模型\n",
    "        return logits\n",
    "model2 = Network2()\n",
    "print(model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a74bdbc",
   "metadata": {},
   "source": [
    "# 6、模型训练"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6845bc2a",
   "metadata": {},
   "source": [
    "在模型训练中，一个完整的训练过程（step）需要实现以下三步：\n",
    "1. 正向计算：模型预测结果（logits），并与正确标签（label）求预测损失（loss）。\n",
    "2. 反向传播：利用自动微分机制，自动求模型参数（parameters）对于loss的梯度（gradients）。\n",
    "3. 参数优化：将梯度更新到参数上。\n",
    "\n",
    "MindSpore使用函数式自动微分机制，因此针对上述步骤需要实现：\n",
    "1. 正向计算函数定义。\n",
    "2. 通过函数变换获得梯度计算函数。\n",
    "3. 训练函数定义，执行正向计算、反向传播和参数优化。\n",
    "\n",
    "为对比增加dropout层和普通定义模型，进行如下设置：\n",
    "1. 构建完全相同的梯度下降方法分别训练两个模型。\n",
    "2. 采用完全相同的数据集以及相同的分割比例。\n",
    "3. 采用相同的交叉熵损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1ffeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化损失函数和优化器\n",
    "# 计算预测值和目标值之间的交叉熵损失\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "#构建一个Optimizer对象，能够保持当前参数状态并基于计算得到的梯度进行参数更新 此处使用随机梯度下降算法\n",
    "optimizer = nn.SGD(model.trainable_params(), learning_rate=LEARNING_RATE) \n",
    "optimizer2 = nn.SGD(model2.trainable_params(), learning_rate=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70123538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    # 定义 forward 函数\n",
    "    def forward_fn(data, label):\n",
    "        # 将数据载入模型\n",
    "        logits = model(data)\n",
    "        # 根据模型训练获取损失函数值\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    # 调用梯度函数，value_and_grad()为生成求导函数，用于计算给定函数的正向计算结果和梯度\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "    # 定义一步训练的函数\n",
    "    def train_step(data, label):\n",
    "        # 计算梯度，记录变量是怎么来的\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        # 获得损失 depend用来处理操作间的依赖关系\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        # 批量训练获得损失值\n",
    "        loss = train_step(data, label)\n",
    "        # 当完成所有数据样本的训练\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d8e9734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model2, dataset, loss_fn, optimizer2):\n",
    "    # 定义 forward 函数\n",
    "    def forward_fn(data, label):\n",
    "        # 将数据载入模型\n",
    "        logits = model2(data)\n",
    "        # 根据模型训练获取损失函数值\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "    # 调用梯度函数，value_and_grad()为生成求导函数，用于计算给定函数的正向计算结果和梯度\n",
    "    grad_fn = ops.value_and_grad(forward_fn, None, optimizer2.parameters, has_aux=True)\n",
    "    # 定义一步训练的函数\n",
    "    def train_step(data, label):\n",
    "        # 计算梯度，记录变量是怎么来的\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        # 获得损失 depend用来处理操作间的依赖关系\n",
    "        loss = ops.depend(loss, optimizer2(grads))\n",
    "        return loss\n",
    "    size = dataset.get_dataset_size()\n",
    "    model2.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        # 批量训练获得损失值\n",
    "        loss = train_step(data, label)\n",
    "        # 当完成所有数据样本的训练\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c19c031",
   "metadata": {},
   "source": [
    "定义完全相同的两个测试函数，通过调用mindspore中的相关函数，使用accuracy以及平均损失等指标评估预测模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e35429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator(): # 遍历所有测试样本数据\n",
    "        pred = model(data)                              # 根据已训练模型获取预测值\n",
    "        total += len(data)                              # 统计样本数\n",
    "        test_loss += loss_fn(pred, label).asnumpy()     # 统计样本损失值\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()# 统计预测正确的样本个数\n",
    "    test_loss /= num_batches                              # 求得平均损失\n",
    "    correct /= total                                      # 计算accuracy\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84443fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model2, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model2.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator(): # 遍历所有测试样本数据\n",
    "        pred = model2(data)                              # 根据已训练模型获取预测值\n",
    "        total += len(data)                              # 统计样本数\n",
    "        test_loss += loss_fn(pred, label).asnumpy()     # 统计样本损失值\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()# 统计预测正确的样本个数\n",
    "    test_loss /= num_batches                              # 求得平均损失\n",
    "    correct /= total                                      # 计算accuracy\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f8c6d5b",
   "metadata": {},
   "source": [
    "训练过程需多次迭代数据集，一次完整的迭代称为一轮（epoch）。在每一轮，遍历训练集进行训练，结束后使用测试集进行预测。打印每一轮的loss值和预测准确率（Accuracy），可以看到loss在不断下降，Accuracy在不断提高。\\\n",
    "相比较没有dropout层的模型，可以看到在测试集中，搭建了dropout层的模型准确率更高，损失函数更小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51406e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 84.4%, Avg loss: 0.563264 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 89.8%, Avg loss: 0.345102 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.285559 \n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCH):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)      # 训练模型\n",
    "    test(model, test_dataset, loss_fn)                   # 测试模型\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ead568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 85.7%, Avg loss: 0.499409 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 90.1%, Avg loss: 0.335517 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test: \n",
      " Accuracy: 91.9%, Avg loss: 0.277547 \n",
      "\n",
      "finished!\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH):\n",
    "    print(f\"Epoch {i+1}\\n-------------------------------\")\n",
    "    train(model2, train_dataset, loss_fn, optimizer2)      # 训练模型\n",
    "    test(model2, test_dataset, loss_fn)                   # 测试模型\n",
    "print(\"finished!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f42acd7",
   "metadata": {},
   "source": [
    "# 7、模型预测\n",
    "\n",
    "模型训练完成后，将训练好的模型保存至指定的路径，方便后续的实验中在使用该模型的时候能够加载模型。\\\n",
    "实例化一个随机初始化的模型，并将训练好的超参数加载到刚才初始化的模型中。\\\n",
    "使用测试数据测试模型的识别能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a40c0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 保存checkpoint时的配置策略\n",
    "mindspore.save_checkpoint(model, \"model.ckpt\")\n",
    "mindspore.save_checkpoint(model2, \"model2.ckpt\")\n",
    "print(\"Saved Model to model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56229791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化随机初始化的模型 \n",
    "model = Network()\n",
    "model2 = Network2()\n",
    "# 加载检查点，加载参数到模型 \n",
    "param_dict = mindspore.load_checkpoint(\"model.ckpt\")\n",
    "param_dict2 = mindspore.load_checkpoint(\"model2.ckpt\")\n",
    "param_not_load = mindspore.load_param_into_net(model, param_dict)\n",
    "param_not_load2 = mindspore.load_param_into_net(model2, param_dict2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57722031",
   "metadata": {},
   "source": [
    "调用加载dropout层的模型进行预测并显示预测结果和真实结果的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "367cb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[2 1 9 0 9 9 1 1 3 0]\", Actual: \"[2 1 9 0 9 9 1 1 3 0]\"\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644b64c7",
   "metadata": {},
   "source": [
    "调用不加载dropout层的模型进行预测并显示预测结果和真实结果的对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93868809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[6 6 7 6 7 1 9 0 2 8]\", Actual: \"[6 6 7 6 7 1 9 0 2 8]\"\n"
     ]
    }
   ],
   "source": [
    "model2.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model2(data)\n",
    "    predicted2 = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted2[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
