{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5732842c",
   "metadata": {},
   "source": [
    "# 基于MindSpore实现强化学习示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46d07a",
   "metadata": {},
   "source": [
    "本实验主要介绍强化学习的相关概念，使用Mindspore实现强化学习示例，以DQN算法为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3344971",
   "metadata": {},
   "source": [
    "## 1、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9b235",
   "metadata": {},
   "source": [
    "- 掌握强化学习（Reinforcement Learning）的相关概念。\n",
    "- 掌握如何使用Mindspore实现强化学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f100fb1",
   "metadata": {},
   "source": [
    "## 2、强化学习原理介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70baac5",
   "metadata": {},
   "source": [
    "（1）强化学习（Reinforcement Learning）是机器学习的一个分支，旨在让智能体（agent）通过与环境的交互来学习最优行为策略，以最大化累积奖励或获得特定目标。\n",
    "\n",
    "（2）强化学习的基本元素包括：\n",
    "\n",
    "- 环境（Environment）：智能体与其交互的外部环境，可以是真实世界或模拟环境。\n",
    "- 状态（State）：环境的某个特定时刻的观测值，用于描述环境的特征。\n",
    "- 动作（Action）：智能体在某个状态下可以执行的操作或决策。\n",
    "- 奖励（Reward）：在特定状态下，环境根据智能体的动作给予的反馈信号。\n",
    "- 策略（Policy）：智能体的行为策略，决定在给定状态下采取哪个动作。\n",
    "- 值函数（Value Function）：评估状态或状态-动作对的优劣程度，用于指导决策。\n",
    "- 学习算法（Learning Algorithm）：根据智能体与环境的交互经验，更新策略或值函数的算法。\n",
    "\n",
    "（3）强化学习的目标：\n",
    "\n",
    "通过学习最优策略来最大化累积奖励。为了达到这个目标，智能体通过与环境的交互，观察当前状态，选择动作，接收奖励，并根据奖励信号来调整策略或值函数。强化学习算法可以分为基于值函数的方法（如Q-learning、DQN）和基于策略的方法（如Policy Gradient、Actor-Critic）等。\n",
    "\n",
    "（4）应用\n",
    "\n",
    "强化学习在许多领域都有应用，例如机器人控制、游戏玩法、自动驾驶、金融交易等。它的独特之处在于智能体通过与环境的交互进行学习，而无需依赖标注的数据集，因此适用于很多现实世界的场景，可以在复杂、未知和动态的环境中进行决策和学习\n",
    "\n",
    "（5）DQN 算法\n",
    "\n",
    "DQN（Deep Q-Network）是一种用于解决强化学习问题的深度学习算法。它结合了Q-learning和神经网络，能够学习并估计状态-动作对的Q值函数，通过使用神经网络来逼近Q值函数，实现了对高维状态空间的学习和泛化能力。 \n",
    " \n",
    "DQN算法的基本步骤：\n",
    "   \n",
    "a.初始化Q网络，目标Q网络和经验回放缓冲区。\n",
    "\n",
    "b.对于每个回合（episode）循环：\n",
    "\n",
    "- 重置环境并获取初始状态。\n",
    "- 对于每一步（step）循环：根据当前状态从Q网络中选择一个动作（通常使用ε-greedy策略，其中ε是一个随时间递减的参数）。\n",
    "- 执行选定的动作，观察下一个状态和奖励。\n",
    "- 将转移（当前状态，动作，下一个状态，奖励）存储在经验回放缓冲区中。\n",
    "- 从经验回放缓冲区中随机采样一批转移。\n",
    "- 计算目标Q值：对于每个样本转移，计算目标Q值作为\n",
    "- target_Q = reward + discount_factor * max(Q(next_state, next_action))\n",
    "- 其中 next_action 是从目标Q网络中选择的下一个动作。\n",
    "- 通过最小化目标Q值和当前Q值的均方误差来更新Q网络的参数。\n",
    "- 定期更新目标Q网络的参数，例如每隔一定的步数。\n",
    "- 更新当前状态为下一个状态。\n",
    "- 如果达到终止条件（例如，达到最大步数或解决了环境），则跳出循环。\n",
    "     \n",
    "c.返回训练好的Q网络。\n",
    "   \n",
    "   \n",
    "（6）gym平台介绍\n",
    "\n",
    "本实验借助了gym平台的环境，该平台由OpenAI公司开发，且提供了一整套与平台中虚拟环境进行交互的API接口，gym 的推出为强化学习算法的研究提供了更好地基准测试平台，同时将各类 环境标准化，使研究员可以专注于算法研究而无需花过多的时间在环境的模拟上。gym 提供一个 step 函数供智能体与环境进行交互，其参数为动作，主要返回值及含义分别为：\n",
    "- state：表示智能体所处环境的当前状态，代表着智能体的观察值即状态。\n",
    "- reward：表示智能体采取操作后从环境中获得的奖励，其类型可能是整数、小数等，但是具体的规模和类型与具体的规模和类型与环境有关，但是智能体的总目标仍然是获取最大的奖励值。\n",
    "- done: 大多数任务都属于阶段性任务，当到达一定条件的时候表示任务已经结束，比如五子棋游戏中的一方五子相连，机器人在路面上摔倒，或者在规定的步数以内没有完成任务，则都代表任务结束。所以 done 是一个判断条件，类型为布尔值，代表当前任务是否结束，如果结束则可以选择使用 reset 函数重置当前任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ea34a",
   "metadata": {},
   "source": [
    "## 3、 实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108f839",
   "metadata": {},
   "source": [
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.0；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d1d2b",
   "metadata": {},
   "source": [
    "## 4、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38991a1",
   "metadata": {},
   "source": [
    "本实验采用Open Gym中的CartPole-v1环境，DQN的实现主要参考了论文：[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa378e",
   "metadata": {},
   "source": [
    "### 4.1 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fa414",
   "metadata": {},
   "source": [
    "在本实验中，需要设置一些超参数，如学习率设为0.01，贪婪度设为0.9，奖励的折扣因子设为0.9等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a841811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7730d48",
   "metadata": {},
   "source": [
    "### 4.2 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb24c5",
   "metadata": {},
   "source": [
    "加载车杆模型、获取动作数和状态数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cd8c4f",
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 科学计算库\n",
    "import numpy as np\n",
    "import gym\n",
    "# MindSpore库\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "# 常见算子操作\n",
    "from mindspore import ops\n",
    "\n",
    "# 加载车杆模型\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "# 获取动作数\n",
    "N_ACTIONS = env.action_space.n\n",
    "# 获取状态数\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379b9e0",
   "metadata": {},
   "source": [
    "## 5、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0197c3",
   "metadata": {},
   "source": [
    "### 5.1 导入Python库和模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47b50a",
   "metadata": {},
   "source": [
    "在使用前，导入需要的Python库和模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ffbc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算库\n",
    "import numpy as np\n",
    "import gym\n",
    "# MindSpore库\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "# 常见算子操作\n",
    "from mindspore import ops\n",
    "# 引入张量模块\n",
    "from mindspore import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9278",
   "metadata": {},
   "source": [
    "### 5.2 定义神经网络模型 Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47ab83",
   "metadata": {},
   "source": [
    "使用Mindspore框架的nn模块，定义一个神经网络模型 Net。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc42fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        # 一个全连接层\n",
    "        self.fc1 = nn.Dense(N_STATES, 20)\n",
    "        # 第二个全连接层\n",
    "        self.fc2 = nn.Dense(20, 50)\n",
    "        # 第s三个全连接层\n",
    "        self.fc3 = nn.Dense(50, N_ACTIONS)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # 全连接层\n",
    "        x = self.fc2(x)\n",
    "        # 激活层\n",
    "        x = self.relu(x)\n",
    "        # 全连接层\n",
    "        x = self.fc3(x)\n",
    "        # 输出\n",
    "        actions_value = self.relu(x)\n",
    "        # 返回输出值\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e4ee5",
   "metadata": {},
   "source": [
    "### 5.3 构建DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf12846",
   "metadata": {},
   "source": [
    "实现DQN（Deep Q-Network）算法的类 DQN，包括网络的初始化、动作选择、记忆存储和学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8bb81",
   "metadata": {},
   "source": [
    "#### 5.3.1 初始化（__init__）：\n",
    "\n",
    "- 初始化DQN智能体。创建两个Net类的实例（eval_net和target_net）。\n",
    "- learn_step_counter用于跟踪目标网络更新的步数。\n",
    "- memory_counter用于跟踪存储在内存中的转换数。\n",
    "- memory数组用于存储转换（状态、动作、奖励、下一个状态）。\n",
    "- optimizer是Adam优化器的实例，用于更新神经网络参数。\n",
    "- loss_func是均方误差（MSE）损失函数，用于计算损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa69591",
   "metadata": {},
   "source": [
    "#### 5.3.2 选择动作（choose_action）：\n",
    "- choose_action方法以状态x作为输入，并根据ε-贪心策略选择动作。\n",
    "- 如果随机生成的数小于探索率（EPSILON），则从评估网络（eval_net）中选择具有最高Q值的动作。\n",
    "- 否则，选择一个随机动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3996df",
   "metadata": {},
   "source": [
    "#### 5.3.3 存储转换（store_transition）：\n",
    "\n",
    "- store_transition方法将转换（状态、动作、奖励、下一个状态）存储在内存中。\n",
    "- 它将转换的各个组件连接成一个数组，并将其添加到内存的当前内存计数器索引处。\n",
    "- 内存计数器递增，以跟踪存储的转换数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ce4ec",
   "metadata": {},
   "source": [
    "#### 5.3.4 学习过程（learn）：\n",
    "\n",
    "- learn方法通过训练评估网络（eval_net）来更新DQN。\n",
    "- 首先，它根据learn_step_counter判断是否是更新目标网络（target_net）的时机。\n",
    "- 然后，从内存中采样一批转换，并将其分离为各个组件（状态、动作、奖励、下一个状态）。\n",
    "- 使用评估网络计算选定动作的当前Q值。\n",
    "- 通过考虑奖励和目标网络中下一个状态的最大Q值来计算目标Q值。\n",
    "- 损失函数用于计算当前Q值和目标Q值之间的损失。\n",
    "- 优化器用于根据损失更新评估网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a690b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net(), Net()\n",
    "        # 更新目标\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        # 存储记忆\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        # 记忆初始化\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        # Adaptive Moment Estimation 动态调整参数学习率\n",
    "        self.optimizer = nn.Adam(self.eval_net.trainable_params(), learning_rate=LR)\n",
    "       # MSE损失函数\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    def choose_action(self, x):\n",
    "    # 输入样本\n",
    "        x = Tensor([x])\n",
    "        # greedy贪心\n",
    "        if np.random.uniform() < EPSILON:   \n",
    "            # 获取所有动作值\n",
    "            actions_value = self.eval_net(x)\n",
    "            # 获取最优动作\n",
    "            action = np.array(ops.max(actions_value), dtype=int)\n",
    "            # 调整维度\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            if type(action) == np.ndarray: action = action[0]\n",
    "        else:   \n",
    "            # 随机获取动作值\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            # 调整维度\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "           \n",
    "        action = np.array(action, dtype=int)\n",
    "        # 返回动作值\n",
    "        return action\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # 更新记忆内容\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # 更新记忆内容\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "    def learn(self):\n",
    "        # 目标参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            # 加载网络参数\n",
    "            ms.load_param_into_net(self.target_net, self.eval_net.parameters_dict())\n",
    "        self.learn_step_counter += 1\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = Tensor([b_memory[:, :N_STATES]])\n",
    "        b_a = Tensor([b_memory[:, N_STATES:N_STATES+1].astype(int)])\n",
    "        b_r = Tensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_s_ = Tensor([b_memory[:, -N_STATES:]])\n",
    "        # 计算现实Q值\n",
    "        q_eval = self.eval_net(b_s).gather_elements(2, b_a)  # shape (batch, 1\n",
    "        # 从计算图中分离，阻止反向传播\n",
    "        q_next = ops.diag(self.target_net(b_s_))\n",
    "        # 保持张量数据类型一致\n",
    "        b_r = b_r.astype('float32')\n",
    "        # 计算目标Q值\n",
    "        q_target = b_r + GAMMA * q_next[0][0][0].max(2).view((BATCH_SIZE, 1))   # shape (batch, 1)\n",
    "        # 损失函数\n",
    "        loss_net = nn.WithLossCell(self.eval_net, loss_fn = self.loss_func(q_eval, q_target))\n",
    "        # 梯度更新\n",
    "        train_net = nn.TrainOneStepCell(loss_net, self.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20f49d",
   "metadata": {},
   "source": [
    "## 6、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b605",
   "metadata": {},
   "source": [
    "完成数据处理、定义神经网络模型Net以及构建DQN之后，开始模型训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29be0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch:  291 | Epoch_reward:  2.38\n",
      "Epoch:  292 | Epoch_reward:  2.43\n",
      "Epoch:  293 | Epoch_reward:  2.85\n",
      "Epoch:  294 | Epoch_reward:  1.31\n",
      "Epoch:  295 | Epoch_reward:  2.92\n",
      "Epoch:  296 | Epoch_reward:  1.68\n",
      "Epoch:  297 | Epoch_reward:  2.56\n",
      "Epoch:  298 | Epoch_reward:  2.5\n",
      "Epoch:  299 | Epoch_reward:  2.02\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN()\n",
    "print('开始训练...')\n",
    "for i_episode in range(300):\n",
    "    # 重置环境\n",
    "    s = env.reset()\n",
    "    # print(s)\n",
    "    # 总奖励\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        a = dqn.choose_action(s)\n",
    "        if a.ndim > 0: a = a[0]\n",
    "        # print(type(a) == np.ndarray)\n",
    "        # 获取训练过程相关参数\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # 修改奖励\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        # 计算奖励值\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        r = r1 + r2\n",
    "        # 更新记忆\n",
    "        dqn.store_transition(s, a, r, s_)\n",
    "        # 计算总奖励\n",
    "        ep_r += r\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            # 从记忆中学习\n",
    "            dqn.learn()\n",
    "            if done:\n",
    "                print('Epoch: ', i_episode, '| Epoch_reward: ', round(ep_r, 2))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        s = s_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ab65fdc43de4932de46c69418c64f6e3769852db39afdc53804a8e5b8ff4a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
