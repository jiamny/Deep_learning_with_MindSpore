{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于MindSpore实现二维线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验将实现基于MindSpore的二维线性回归，包括人造数据集、模型构建、模型预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、实验目的\n",
    "- 掌握如何使用MindSpore实现二维线性回归模型。\n",
    "- 了解如何使用MindSpore的Adam优化器和MSE损失函数\n",
    "- 了解基于MindSpore的模型训练和模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、整体模型介绍\n",
    "- 二维线性回归原理\n",
    "\n",
    "在机器学习领域，线性回归模型记为：\n",
    "$$y=w_0x_0+w_1x_1+\\cdots+w_nx_n+b=[w_0 w_1 w_2 \\cdots w_n][x_0 x_1 x_2 \\cdots x_n]^T+b$$\n",
    "可以统一形式为：\n",
    "$$y=\\sum_{i=0}^{n}w_ix_i+b=w^Tx+b$$\n",
    "\n",
    "\n",
    "并且我们定义损失函数来度量模型一次预测的好坏，即预测值$\\widehat y$和真实值y的误差，线性损失函数一般取$L=\\frac{1}{2}(y-\\widehat y)^2$，平方损失函数的几何意义是欧氏距离。\n",
    "\n",
    "之后我们便可进行模型训练，采用梯度下降方法求模型参数w，使损失函数最小。\n",
    "\n",
    "梯度下降法顺着当前点梯度反方向，按规定步长$\\alpha$进行迭代搜索，对第i个模型参数进行如下更新：\n",
    "$$w_{i+1}=w_i-\\alpha \\frac{\\partial L(w)}{\\partial (w_i)}$$\n",
    "因为\n",
    "$$\\frac{\\partial L(w)}{\\partial (w_i)}=-\\sum_{j=0}^{m}[y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b]*x_i^{(j)}$$\n",
    "所以\n",
    "$$w_{i+1}=w_i+\\alpha [\\sum_{j=0}^{m}(y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b)*x_i^{(j)}]$$\n",
    "对每个模型参数迭代训练直到收敛即可。因此二维线性回归的自变量为两个，模型便为：\n",
    "$$y=w_0x_0+w_1x_1+b=w^Tx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们需要定义一个模型Net实现二维线性回归功能。<br>\n",
    "可以调用MindSpore的nn.MSELoss计算预测值和目标值之间的均方误差。<br>\n",
    "调用MindSpore的nn.Adam计算最优参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=2.0；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、数据处理\n",
    "#### 4.1 数据准备\n",
    "为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。任务是使用这个有限样本的数据集来恢复这个模型的参数。我们需要生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。 合成的数据集是一个矩阵$X∈R^{(1000×2)}$。我们使用线性模型参数$w =[2,-3.4]^T$、b=3.2和噪声项$\\epsilon$生成数据集及其标签：\n",
    "$y=Xw+b+\\epsilon=w_0x_0+w_1x_1+b+\\epsilon$\n",
    "\n",
    "$\\epsilon$可以视为模型预测和标签之间的观测误差，我们假设ϵ服从均值为0、标准差为0.01的正态分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random库实现了各种分布的伪随机数生成器；dtype是MindSpore数据类型的对象；mindspore.ops提供对神经网络层的各种操作；pyplot是常用的绘图模块，能很方便让用户绘制 2D 图表；mindspore中的Tensor是张量，可放在gpu上加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    print((num_examples, len(w)))\n",
    "    # 生成X\n",
    "    X = np.random.normal(0, 1, (num_examples, len(w))).astype(np.float32)\n",
    "    # y = Xw + b\n",
    "    y = np.matmul(X, w) + b \n",
    "    # y = Xw + b + 噪声。\n",
    "    y += np.random.normal(0, 0.01, len(y)).astype(np.float32)          \n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "mindspore.set_seed(1)\n",
    "true_w = np.array([2, -3.4]).astype(np.float32)\n",
    "true_b = np.float32(4.2)\n",
    "# 人造数据\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 数据加载\n",
    "数据features中的每一行都包含一个二维数据样本，真实值labels中的每一行都包含一维标签值（一个标量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[ 1.6243454  -0.6117564 ]\n",
      " [-0.5281718  -1.0729686 ]\n",
      " [ 0.86540765 -2.3015387 ]\n",
      " [ 1.7448118  -0.7612069 ]] \n",
      "label: [[ 9.533557]\n",
      " [ 6.794138]\n",
      " [13.751566]\n",
      " [10.271619]]\n"
     ]
    }
   ],
   "source": [
    "print('features:', features[0:4],'\\nlabel:', labels[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGdCAYAAAA8F1jjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABCL0lEQVR4nO3df5AcdZ3/8dfsj0yWNdlNMCQsbpawyipgftyeEMTSgHwTUnv8uKrTk6to8Kw7TSERMWq4E/eih4mYurPwqMj9grPQgHVXEKwUJlwk5AQCGrOEoMQihRvIEogkm022Npv9Md8/ls/Q09vd0zPTPd0983xUbZHMTnd/ejba731/3p/3J5XJZDICAACIoZqoBwAAAOCGQAUAAMQWgQoAAIgtAhUAABBbBCoAACC2CFQAAEBsEagAAIDYIlABAACxVRf1AEo1Pj6uvr4+TZs2TalUKurhAAAAHzKZjE6ePKmWlhbV1LjnTRIfqPT19am1tTXqYQAAgCK8+uqres973uP6/cQHKtOmTZM0caPTp0+PeDQAAMCPgYEBtba2Zp/jbhIfqJjpnunTpxOoAACQMPnKNiimBQAAsUWgAgAAYotABQAAxBaBCgAAiC0CFQAAEFsEKgAAILYIVAAAQGwRqAAAgNgiUAEAALFFoAIAAGKLQAUAAMQWgQoAAIgtApUSPLC7V1ds+IUe2N0b9VAAAKhIBCol2LTzoA73D2nTzoNRDwUAgIpEoFKCVUvadV5zg1YtaY96KAAAVKRUJpPJRD2IUgwMDKipqUknTpzQ9OnTox4OAADwwe/zm4wKAACILQIVAAAQWwQqMcfKIgBANSNQiTlWFgEAqhmBSsyxsggAUM1Y9QMAAMqOVT8AACDxCFQAAEBsEagAAIDYIlABAACxFWqgsmvXLl177bVqaWlRKpXSI488kvP9m266SalUKufrmmuuCXNIAAAgQUINVAYHB7VgwQLdc889ru+55ppr9Prrr2e/Nm/eHOaQAABAgtSFefLly5dr+fLlnu9Jp9OaM2dOmMMAAAAJFXmNys6dO3XOOeeoo6NDq1at0ltvvRX1kBAAWv8DAIIQaaByzTXX6Ec/+pF27Nih7373u3ryySe1fPlyjY2NuR4zPDysgYGBnC/ED63/AQBBCHXqJ59PfepT2T9/8IMf1Pz589Xe3q6dO3fq4x//uOMx69ev17p168o1RBRp1ZJ2bdp5kNb/AICSRD71Y3XBBRfo3e9+t15++WXX99x+++06ceJE9uvVV18t4wjh14rFbXpq7VVasbgt6qEAABIsVoHKa6+9prfeekvnnnuu63vS6bSmT5+e85VE1HAAAJBfqIHKqVOn1NPTo56eHknSK6+8op6eHh06dEinTp3SV7/6Ve3evVt/+MMftGPHDl1//fV673vfq2XLloU5rFighgMAgPxCDVR+/etfa9GiRVq0aJEk6bbbbtOiRYv0zW9+U7W1tdq3b5+uu+46XXjhhfrc5z6nzs5O/d///Z/S6XSYw4qFVUvadV5zAzUcAAB4SGUymUzUgyiF322iMTHdZApcqR0BAETJ7/M7VjUqKE2+uhemmwAASUOgUkHyBSJMNwEAkibSPioIVr7eJSsWtzHlAwBIFGpUUBDqXAAAQaBGBaGgzgUAUE4EKigIdS4AgHJi6gcAAJQdUz9wRft+AEBSEKhUIepMAABJQaBShagzAQAkBTUqEWCJLwCg2lGjEmNMvQAA4A+BSgSYegmWU3FwkAXDFB8DQHQIVDyE9YBasbhNT629quKmfaJ6oDtlqILMWpEBA4DoEKh44AFVmKg+L6cMVZBZKzJgABAdimk9xL3oNW7ji9t4AADx5ff5TaCSYFds+IUO9w/pvOYGPbX2qqiHAwCAb6z6qQJBTUlQLBpf/GwAVDsClQQLqig3iNqSuD1Q4zaeYlEnBaDaEaggkMxMXB6oJkDZuO2ADvcPqXvL/kQHKxTyAqh21KggEHEppDV1O80N9Tp5ekRjGVHDAwAx5Pf5XVfGMaGCrVjcFouVPquWtGcDJkk5fwYAJA9TP6gopm5HUiwyPACA0hCooCLFpWYGAFAaAhVUJLci1EpZDQQA1YJApcLwIJ7gtnSbTAsAJAuBSoVJ6oO4mACrmGNY7gsAyUKgUmGS9CC2BhrFBFjFHFOpO1cDQKUiUEkIv9mDsB7E+a5fTHbDGmgUE2AVcgxTYgCQTAQqCeE3exDWAznf9Z2+n28s1kCjmACrkGOSOiUGANWOQCUh/GYPwnog269vD0KcxpdvLOWchknSlBgA4B200K8w5Wplv3DddvUPjai5oV493UsjHQsAIHlooV+l4tLKHgCAIDD1U8VKqWdZs6xD5zU3aM2yDtf3UBcCACgVgUoVKyWQ8FNfQl0IAKBUBCpVLOxAgp4l72B5NAAUh0ClisUtkCj2YV6urralYBoMAIpDoIKyyRccFPswD6OrbdCBjDV7RXYFAPwjUEHozIN547YDnsFBvl4tbsLoaht0BsSavSK7AgD+0UcFobtiwy90uH9IzQ31akzX+e6rYo47r7lBT629qgwjfUeYPWDoLwMA/p/fBCpVJoqHZLHXdDqOh/xkfCYAkohABY6izFL4ke+hax3/qiXtPKAV/58pADjx+/ymRqWKPLC7V4PDo2puqI9FbxOnGpR89RurlrSruaFeg8Oj2ZqXjdsOBFqcmrRiV/rVAKhkBCpVZNPOg+ofGlFjui4WGQhroGHke+iuWNymxnSd+odGJEnnNTdIUqDFqUkrdo3bMnMACBKBShVJwm/eXg9dk+nobJuRbd//1Nqrsu38ne4rX3bE6fvmc+psmxG7zErSsj0AUCoClSoSl9+8zcP2oxfOyrtfkJXJdOzpPZ6txbhiwy8kyfW+8mVHvL6/6/dHi86shBVQJC3bAwClIlCpQlH/Vm4POPwGTvaMkDmPV41KviyS0/fNeSUVnIHy6hkTxOeehKwYAAQp1FU/u3bt0ve+9z3t2bNHr7/+uh5++GHdcMMN2e9nMhl1d3fr3/7t39Tf368rrrhCmzZt0vve9z7f12DVT+GiXiUS1HLaB3b3auO2AzoxNKKMFNj9lDI+a88YY82yDq1Y3Ob5vbDGAwBxFYtVP4ODg1qwYIHuuecex+/fdddduvvuu/XDH/5Qzz77rBobG7Vs2TKdPn06zGFVvah/Kw9qCsoU1mYk1aYU2P3Yx2fNhOTLipjPds2yjmzRr8mqdLbNUG1KGh4dU//QSM73vDDdA6Ca1YV58uXLl2v58uWO38tkMvr+97+vb3zjG7r++uslST/60Y80e/ZsPfLII/rUpz4V5tCq2orFbZH9Zh50dsBPL5VSr2kPFKx/tp/X/tma70vSnt7jGstI6bpapetqs+P3EuaScjI1AJIgshqVV155RUeOHNHVV1+dfa2pqUmXXXaZnnnmGdfjhoeHNTAwkPOF5AhzD52wrmnNQFn/nO+89rFZsy093UvV0700b4AQ5pJyMjUAkiCyQOXIkSOSpNmzZ+e8Pnv27Oz3nKxfv15NTU3Zr9bW1lDHiWCVa9rJOkXjtdzYT4GrNeCw/rnQeylmyivMzyvqKUAA8KNsLfRTqVROMe3TTz+tK664Qn19fTr33HOz7/vkJz+pVCqlhx56yPE8w8PDGh4ezv59YGBAra2tFNMih1PBsN/XAADhi0UxrZc5c+ZIkt54442c1994443s95yk02lNnz495wuwc8oW+H3NKuql3ABQ7UItpvUyb948zZkzRzt27NDChQslTURXzz77rFatWhXVsFAhnAqG/b5mZa3joOAUAMov1IzKqVOn1NPTo56eHkkTBbQ9PT06dOiQUqmUbr31Vv3jP/6jHn30Ub3wwgv6zGc+o5aWlpxeK6hubhmNcmU64lrH4XX/ZIEAVJJQA5Vf//rXWrRokRYtWiRJuu2227Ro0SJ985vflCR97Wtf0y233KK//du/1Yc+9CGdOnVKP//5zzV16tQwh4UEcVuZUq4VK1FvO+AWdHjdP6t5AFSSUAOVJUuWKJPJTPq6//77JU0U2H7rW9/SkSNHdPr0af3v//6vLrzwwjCHhIRxy2iUO9PhN0vxwO5eLVy3XQvXbQ8ko+EWdHjdf1yzQABQjLKt+gkLLfSTrZxNx4q5ljlmcHhU/UMjeVcHmVVEUjAt/f2OmeZtAJIm9qt+ACmYaQq/Le6LuVahGxSuWtKu5ob6kjrJWu/B79QT0z0AKlVkq34AKbcFfrHcWtzbH+6FXMtkKDrbZmSP9ZOpCGJ7Auuu0NbxemVMgvgcvZCxARAVpn6QGG4PS+vrkvcD3e81/E71hMFpDJIibUxHYzwAQWPqBxXHnjkxUySSHFvcl3INaWKq5+zGKWq/fatWb94bzE34YO5hzbKObOv/sDYm9IsCXQBRIVBBYtgfll51Gas37y0qwLBuHPjU2qv0Yt8JjWWkrfv6PI8rpndJvmNMwLKn93h2Y0JJkfRIiXqZNoDqRaCCxHDbjdjpt/yt+/p8BRj5rtE1v0W1qYn/enELmvwW93o1trNmU/wUzdLwDUAlIVBBYnn9lu83wMjn7hsX6eD6Lt194yLP97kFTV6BhfUYt2mtjdsOZLMpZsfm5oZ6DQ6PugYirAACUEkIVBBLpWYFvAKMoDMOXiti3AIY+zH2AMRtWfSKxW1qTNepf2jENRChngRAJSFQQSwVM5XixOn9TucuJXjxymC4ZX3sx9gDEHutjPX4fIGIuaYUTj0LU0sAyolABbHkdyol30PTKYhwOrefehG36xWTwXA6xvqa17RWOZrAFdI4j8AFQJjoo4JEsU+ZmP4etSlp3fWXTHp4Wxu37ek97tpf5YHdvdq47UD27249VBau267+oRE1N9RrzbKOSVM+YTRGK/acpYzFq2+K28+AHisACkEfFUQqrN+ynVb+1KaksYwcsyHWJb5e2QXr1Ivkr12+ySx0b9mfvV6QWQxrQW0xK328Mi/5fj5eWaJCVl8BQKkIVBCKcq08WbG4Teuuv8R19YzR2TZDtSllW+I78aoLMUwTtjXLOiYFSdZzFPPQto/da58he6DhFDQVci27Qvqm0GMFQJiY+kEootobxum6qzfv1aPPT/RTsU9PlDrOUqZlzFTTmmUdWrG4bdI0ldd0lX265YHdveresl9jGX+7NjtdvxSFfg7sHQSAqR9EKqrfss1SXzMNJOU2fSukz0m+6ZFSHrabdh5U/9BIzjJjM/at+/p0uH9Ie3qP53yG1vHYMzf2zFI+fpY5F3o/hWTQ6PUCwC8CFVQc+0Owa36LUpIa6msnvddrqibfw7SUh63pm9JQX6tjg8NauG57NvAZy0i1qclBlalV2bjtgGMgWGhwWOg0lVfgVui5qGsB4BdTP6g4TpmOYlam5MuYeH3f72ojMy5JOXU2q5a067lXjulnz/dpan2t/r7rA9kutc0N9erpXlrox1LydAurewAEye/zm0AFFcNP4FCumgjrsmmvuhGvWpH227dq7O3/ddqDmFICDbel3PlQVwIgSAQqqDpx+o3fb0bF6Rjz3tWb9+ZkVKzHFxM0FFpwCwBhIlBB1Ynzb/x+ApdCAi3rewvJtMT5MwJQXVj1g9gLuilcsSuNytEC3hTe/uz5vmxBrJ1Trxe3sXntvOyFnicAkoZABZEp9xJVt4e+dTVNWEGLCULqalKuY/vZ830ay0i7fn80+7pbIzdrwBH0Cppq3bunWu8biDsCFUSm3EtU/QRGpfRV8bKn97jGMlJjui7b2dZ+Xac5WKfut3ZBZ0mi6nESdaBAbxcgnghUEJkgH7B+HnJugZG9LX6xfVW8uLXnX715r9pv36qzG6eouaE+u9mhUWgjt0If9n52g3Z6TxhBRdSBAr1dgHiimBYVoRwrfsIoRDVLkGtT0sH1XSWPqdDPwc8uyYPDo5N2ky70On4+Owp9gepCMS2qivltuLNtRmjTB/kyQMVkGS5uacr5b6HntNewFNMhtrmhXoPDo1q9ea/jRofS5E0R/X7ehewATaEvACcEKiirsOoQzENuT+/xyKYPipm6eGvwTM5/Cz2nvYbF+rD3u1eRJPUPjWT3GLLvBO20m7Tfz9sr2AEAPwhUUFZh1yGUq87ggd29Wrhue3aPnnzXdqvzGBweVXNDvet4/dzPtKn1aqivydkzSPK/V5E0EUR0zW+ZtNGhU4bDeu+dbTMmjc9p80RT/2PdLBIA/KBGBWVVKXUI9j16nOo7rK3xTVBQSp2Hkw/c8XMNjYzlvGbO5/ZZF9M1t5B7t9+XV60LgOrl9/ldV8YxocL5CUJWLG5LZIBiv7dVS9qzgYjbCqH+oZHsn63dY62BgtvxTtfubJuR7bFi9gU6bQtSUpbzmc95086Deu6VY9mgxJpJcdt/yCuQsd67qVGxvsd6r+b6h/uH1NxQn1PXkvRgFUB5kFFBYOK0107Q7Bv6SfIMytw2Gyxmvx37BofSO8eu3rxXW/f16eKWJr01eGbSeMyxKUkZKbv82WvsbhsqOt2Tn595qSuTAFQmVv2g7Cq5D8WqJe1KaeLBvXHbgWyWYOO2A44FqysWt2X7oZiOt9JEcGOWI5vPafXmvZq3dqs+cMdjjvUb5nPtmt+S7bVijr37xkU6uL5Lj97yEcd6EqeOuPbaE3v9jPV61p+nyRL1D434WmVkzispZ7rL1LWEuUILQOUgo4Kq5TRV5TV9tXDddvUPjWSzEhu3HdCJoRFl5F2rIXlnJUwvFbmcp5R7MmNobqhXY7qupM0Qzdi97tl6fXtdjv06ZFaA6kZGBRWp1OXN1uOdVsV4rZSxdrBdsbhNjek6ZZSbHbFeZ3B4VA31NTkZEHNc/9BIdgXMlLqJ/xk21NeU3A3WPn6vJcaGU1bEei1rZqSne6m+fYN7p1zr9e3nzfd3AHBCRgWe4rZKp9Tfwq3HW4s+/WRU7Lze66fjq1PWwev4fPfuVhdT6H3ZryXJ8zOznr/YlUTFjjPo4wGUj9/nN4EKPMUtPV/Mg8h6jORdBFvucZpi2K75Lbp03sxJ47Q++PON/QN3PKahkXE11Nfod99enjMOszTYPgXkZwmzdaVRIYFVMYIMROPw7xWAOwIVBKISfkO1r9jxcx/lum+3zIV5yFrrYnq6l3qea97arcpoYonyKxu6co5vqK/VzMYp2YClob5GZ0bHNaWuVkMjY66Znzse2Z9dLdTTvdS1rsctk2M/X76sCxkVoHpQo4JAVML+K/Y2836YvWnMA9hJENsBWOs07PvnrN68Vyfe7sXix7ULWlSbmvivXbquRk+tvSpbZ3N6ZFxjGen020GK6e9i3+vH/luM078HsxqoMV3n+e/EZGPsrfqNIIKMSvj3CiAXgQoq3orFbVp3vXsBqOFnbxxrgWn3lv3ZB679WD9BjP3BbN8/Z+u+vmyxrlnq7MUsVb77xkXZ1+bOPCvnv+Ya1y5oUUrS1Pra7PWdCnGbGyba8w+Pjue057fyUxRr3S7AvuzZ8CpkXr15r9pv36rVm/e6nr+QoDGsPacABI9ABVXBz2/a1geldYWP0/ft/VDsD1k/exq5vcfeN2Xa1HrPe/N66L7YdyLnv+a9l86bqZbmBg2NjE1aIWRdodTTvVQzG9MaGhnLrlSy8/vZ9g+N6OTpEV06b6bj+70Cnq37+jSWmfiv2/kL2UMq7D2nAASHQAV4m/VB6fTwdZqmMTUvxSy97WyboZSkY4PDOUGGufbdNy7KWcrsxmuaqmv+xHRQ1/yJ6SD78uHmhnodGzyjheu267lXjrl+LvZGc1b2TNPCddv1gTsem7RhY77pN6+Ax34fTmMsZKlzviXZAOKDYlogj2JrJ/IVj9obwhWzXNpa8NpQX6OZjWnPItWzG6foxb4T6prfortvXJQttpU0qWW+X04FwYb1XH7uJYil4cVixRBQXhTTAgHxO03gVIxqLR61t9u3Zyqs17G3n3d7GJuC19qUlK6rdR2nOfeLfSc0lpF2/f5oTpAiSTWpVE7WxK3uZvXmvTmvmzb9nW0zcupa7BmYfFNEhUzHhDF1QwM6IJ4IVBAbYaTeg16Z48X+8DQP8ItbmnKyDeb7pgakp3tpdvqouaFeg8Oj2emcfA9i6xSUqatxGqfTWPptK4pGxjM6efqd19zqbuyrdvb0HtdYZuK/5p7+vusiNaYnb87u9fMoJFAII6hgxRAQTwQqiI0wfksO4px+HmDWVS3m4Wke4G8NnslZGuz2cLW215eU8163B7x1bNY/29/vNBazTWFDfa0a6mslvbPp4sJ123VscFgN9bUaHB7V6s17XVftOAUN5nM3mxcaTvU0frNHbvcNoLIRqCASTg/eMH5LLlc636mXiHVKRPL3cHXam8e+FNrwyk647fljXdFj9uz5+64PaGbjFEkTU0jSRLZlaGRcZ0YnVvs8+nxf9v7uvnFRzn24FR777V3jFtQAgBSDYtp/+Id/0Lp163Je6+jo0EsvveTreIppk6nSChedCmfd2s0Xyq2zrt/9hArpxGta5g+Pjr39nZROj4xl62DWXX+JJH/bEDhtXeBUWGwCsbGMsi3+S90zyI+oi3eBapeoYtqLL75Yr7/+evbrl7/8ZdRDQsgqrXDR3qzNafdgv+yZEvtSaPP9zrYZrue3Zzny1eqYGpmt+yYyJzMb09n+KU0N9TnXNwGYvTjYizlmT+/xSdkXa0M+SZ7da4MUdfEuAH8mV7tFoK6uTnPmzIl6GAiZUyfWSmNfYlzMPVofiuYc9pb1Zgmwn0yNNWNhzun1HtPEzlzL+mfrPQ4Oj+aM03ou60aI1p2ivepzrJsk2jdjzHd/xWw6mW9Mxb4XQLBiMfXzve99T01NTZo6daouv/xyrV+/XnPnznV8//DwsIaHh7N/HxgYUGtrK1M/CVDO6Z6kpOqdNvQrpN+I5P1gtgcgXfNbcqZU7EGF28aN1s0RzS7Kbjs7m0DKvlNzIZ9JIYFHvo0dvc4f538bQKVLzO7Jjz32mE6dOqWOjg69/vrrWrdunQ4fPqz9+/dr2rRpk97vVNMiiUAlAcr5gEhKDYy96ZvTWL0+N7fdld0CEHvdzDu7K09uFme97sZtB7LXaUzXTfpsrXU0bsGQ3597KYGH5BzYWN8TVO0QgNIkJlCx6+/vV1tbm/7pn/5Jn/vc5yZ9n4wK/EjKb81OGRU7r6DLLdBwe91eNGuWQqckffuG3EyKuW5zwzt7Da1Z1qHnXjmmrfv6st1tpYlNAx99fmIfHvs4/QSNboW3u35/1POzyXcu66op03XXqQMwgPJLVDGtVXNzsy688EK9/PLLjt9Pp9OaPn16zhdgl5Q+G/amb5JzMa1pBGcvXDW9Wdy60qbrah0/B2uQIkkZTV5GbIp4JeUsvbY2eDOsf5Yyar99q677wS/zFv0a9rocU5hsX/Lth1OjOmvtTaH/NtgDCIhW7AKVU6dO6eDBgzr33HOjHgoQOLeHnvV1pxUmJ0+PZAtTV2/eq/bbt2r15r3Zh669mdyaZR3ZTIj1WtZCXNO+/7oFLY6BhPXc1s0LnQIP0zdFkg73n9ZYRtp3+ETOSh9JBXWl9QrQvD5Ttw0inWpv/GDFDxCtyKd+1qxZo2uvvVZtbW3q6+tTd3e3enp69Nvf/lazZs3Kezx9VJAkbtMg1tftUxP2+g8zxVKbkg6u7yroWk7TIn4KV/PV0jywu1d3bv2tTo+Mq6V5qo6cOK2LW5r01uCZSfdRSG2IdfrJrTA37HqkpEwjAkmTmKmf1157TTfeeKM6Ojr0yU9+UmeffbZ2797tK0gBksatt4r1dZPJkJQzdbLu+ktypli65rc4XsOrz4r93NY9hbwyB9YNFDvbZkzKjGzaeVBDI+Oa+K0npYPru/ToLR/JmWKx3qPf6RTr9JPX2ILsyWMfW1KmEYFKFXkflQcffDDqIQCB8voN3K23itPr9n4p1v2EvIpLzXGDw6OOGwNa39P8djO3zrYZevy3R5TSOy3/7fdiVhWZDIapJzHjaqivUbqu1levFFPcunHbAV/Zio9eOMu1r0rQPXns9TIAohV5RgWIkyAKJ4OqabBnCpz2E/I6TsqfhTB7Cu3pPZ7NiOz6/VFdseEXWr15b84eQ06ZGhN0mG621qJgO3P8xm0HssWtZoxuXW69Otq6nT/fzy7f+yqtazKQdAQqgEUQQUZQDzr7lIPf87oV2Fof0E7TS2ZqR3qnjb11tYw9aJDk2M3WjTVDZKayzBjNNe0bE7rds1Ow4fdnl+99pU71sEoICBaBCmBhfzAW89CxBwFBPbD8ntesCnrulWN5d2A2D+1dvz+qxnSd1izryK7ymVJXo4b6Gk2bOhG8OGV4TJBiX1HjtTu2dWdoa1DltNuy2z07BRvm/E41NFb2Xa2DxiohIFgEKoCF/bfpUh46xRzrJzAy57VnH8yxP3t+IhOydV9fzjFjb6/vsz6gnaaJzL0PjYzr9Mh4dlm0W4bHGqRYp3fs9+6VqbBuTOiUmbF/lk6ZFqeNIZ049YEJElNHQLAIVAAPpTx0ijnWT3BjepbYsw8mOKirSWWXMhvW4MT6gHabJjKm1tcUtFLHPr1jzW742cHZKZCxFhGb8XkFPfk+90J+LqVk1CjEBYIReR+VUtFHBXFUbO8Nv8c5vc++78/qzXuzre5NlsFt00E/57f3K/HTp+WdVv61OjM6lm1j/9Taq7Ljs/dbsYty36ak7BkFJFFi+qgAlcieGfH7m7nfOhSn39pNVmTNsg5JyhbDbt3Xl7c7q5/eIfZMhFOth1s24fTI2KSiWzM+08HWLYvkt/YkDEzjANEjUAFC4FR4WkjgUmh9i3WzwY3bDmjhuu26uKUpOwWUbzrCXM9tmbA0OQjxqvUwBb1zZ56l85obdO3bbfqtgVLX/BbVpqT55zV5BgN+a0/8YEUOkDwEKkAI8i0tzheIFPqbvDnf1n196h+a2BforcEzOri+K7vDsRtrDYjk3nvFvNc86L3GaLIlL/ad0FNrr9Kl82ZKkp575Vi2R8ue3uNad/0lkzrYugkiu2HqeMyO1fmwggeIHoEKUAaF9kQptCDTnK9rfku2H4rTud36j5hGcm5Ftdb32nc5liZPU5lsiSnoNcc9+nxfzn8LCQCiKFJl6geIHsW0QBVxK4A1GQZra36nglo/RbZOrLtCG/m2AsjHrfDYqyC53BsMsqEh4I5iWiACUdRAFHJNt/4jjem6bL8Uw2naY8XitmyXWnM9Pw3UTDbkugXvLJnOtxVAPm7TMl7TNWE14yt0jAD8I1ABAhTFgylfIaxT63x7gOAUwLhNe9jvcdfvj2osM/FfO3sQdfeNi/SPNzg3dis0yPOzE7Xb+cv1c2LqCCgdUz9AgMqZ6reu9NnTe1yDw6PqHxqZNAUTdC8Qt14ppn+L9fvWXZob03We0zTmvV59WUph/RzM9dzOG8bPkWkgIBdTP0AEgi749Moy2DcJdCqEfWB3r44NDiul4Pa2sd7j6s17dWJoRA31NVqzrEMP7O7VHY/sz2Z48u3kbM1sOGUfgsx8WM+fbwrIet2gpvOYBgKKQ6ACxJjbw2315r3q6x9SQ32tZ1v5TTsPamhkXBm90+8kiAevdV+hjKQzo+NasbhNG7cdkDVFa8b00QtnZetY3JY4+2kyVwq3z8cpILFeN6gAg2kgoDh1UQ8AiIO4puWtUxRWW/eZAGHMc7yrlrRnV/TYe7h0b9kvSTkrepxW/zgx5zCt8a37CklS6u1zGNbmcKZx28ZtBxyng6zMDstGvp+TfTos38/T+vlaAxKngKbUAMN+L2GJ679loFhkVADFNy3vNpVk71NiOLXC7+leqp7upTk9XJw2NTT9VMzqH6/Mi1np8/8ump3TVM5MP337hkskyTFDkW86yEu+n5O18Z2fxnWSsp+vW8bDz3ReXDrePrC7V91b9sfy3zJQLAIVQMlLy9994yLHrrNeD3Lrw3nd9ZNX3qxa0p5tFtfZNsPzgefVPt+wdoG1PuyddmzO96A33+9sm5FzzMJ127Vw3fbscdbGd34b1xleAckDu3v1gTt+rnlrt2r15r2e5zPbBzi9L2ybdh6ctKcSkHSs+gHKoFzpeK/rFLL6x7zXbbdlt+tYr2FWIZnGbtb3W3d2vvvGRXnH5vR985qkglc0+fl5WKeRzJYA0kQQcHB9l+v5urfszwYL9vcVqtB/N0z7IElY9QPESNhTS05TGdbX8+3NY2fNTFibuxl++rFYd3M22ZU7HtmvB3b35uzsbN1ryG1sbn1eGuprilrR5Gc6xzqNZIKUlHKn25w+d7dpuWIU+u8mim0GgLCRUQHKIOzfdN0yEqX2UHFruW+9Fz/3ZnqtSBPZD5Ol6Jrfki2uLWaM1vFZz+m2EePqzXv1s+f7NLW+Vn/f9YG8dSf5CnPN9d36xJTK+tlKIluCikJGBYiRsH/TLbRLqx9umQ77b/nWVURuNSZrlnXkbJZorbHx00k23313ts3Qo8+/k6VxO96slhoaGcubpTA/s7tvXOT6s8tXGFxqka31301cC76BsJFRAWImDvUsknsnV0mTMiqmLiOo7rf263ud03p9SbpuQYsunTczZ0xm/Gc3TtELh09kMyr2eymWn5qdUj8X6k9QacioAAll/805rKWv+X5Dd2t6Zs8OrVjc5riKyC+3+/ObDTIrXSSpob5Gl86bqY3bDmgsM1FTYh3/W4Nn9MqGLv3u29e4ZikK/by9AoiwG9YB1YBABQhBKcGF/eEWVso/30PU+mDMt0Oy9b2F3nu+TRX93EdtauLPQyPjOZ9TU0O9Z4+UQtr2m/tavXlvzjj97NZMcAEUj0AFCEEpwYX94RZWj5dCHqJ++qYYfu7daTWSJMfaF6+eMGZH6HXXX5JTA2NWHH30wlmOq6HMOZwyIW5Bmb2ZnKnJKVcPnrg0lQPKjUAFCEGlpfyt95PvgWkaxw0Oj7q+x97G37of0NmNUyY1d3M73gQx9g685jMzK4qcgh23aR+zHNkelFmXbFs7+xby8ykl2KCYFtWKQAUIQRDBRZx+gy5k9cmKxW1qTNdlW/E7cWrjv+v3RzWWkV44fCJnV2iv1Tb5AievoMnpe/bOrvbMjVkFVGxNTinBhlfwG6d/K0DQCFSAmIrqN2g/GZN8D2k/9S9uD/up9TV5lyuboEGSY6t/ayM2t6DJKaAy4zbdeN1+BsUGoqVk2ryuSbYFlYzlyUBMRbUcNcgltW6c7s3tfs3rpiW/dSWSec20+pfk+l6nz9HrM7buJv3RC2fl3Y05yuXD5bw2y6QRFL/PbwIVoAIUsndNvgdMKQ8ivwFIMfsOWbu/mgxCQ32NTo+MZ/uimNe9OsXmuz+nwEhS3vGWI8CLg2q5T4SPPipAFfGT+vc7PVBKfY3TNZxeyzcF4rQqaM2yjuyD0XTMTdfV5nSaNXUn5hrFTJOY70vvNIvz6p5rdnD2Kv6tJOVa5QQYBCpABQiibiSscTi9li8YsjeYM1kUk+3oHxpRY7puUmt+v4W8Xp+DU7DjNl4zlv6hEc/i36iEUWQbh1VoqC4EKkAF8PPwKMcDJqhreDW9s37PvizZemxn24xJD2k/01pOwY79gW/+3tk2IydQchLlihyKbFEJCFQABGL15r1qv32rVm/em/O634el06oep6Z3+YIhrx4qfseSrzuw+fue3uM5gZJTUFLM/ftRyKaNTNMgySimBSCp9NUc7bdvze6v02IJKvye120TxGKKg60rdtYs68iOY+O2AxoeHVe6ribnda9rrd68Vz97vk9T62v0/y6aoz29x9XZNiO7Ckh6Z2NDa5HvmdFxdc2f2CCx0Pv3U6RKUSuSjmJaoMoVu+dOsdMEXfNblJKUUW4rfHsGxG1c1tb1pRYHW+tYzHXNa2dGx3KmdfJda+u+PmUknRkdz2ZprPUo1r2KTJHv6ZFxjWUmjvXKADkVDfvNfpAtQbUgUAEqVKGBR6kPvrtvXKSWt5fyms6ufsZlHtaP//aIxjITHWoLLQ62Bz9OXWetLfCt587X8t+0zO+a3+JZLCxJ/UMjkqSp9bVKvX2sF69dqe3s90hRK6oFUz9AhYqiMVcx/VzMFIbJxjQ31GvNso6Cxu40DVLIVFIQ0yhO/VfynauQn1EhY6QpG5KAhm8AEsE8VE3dR2fbjOzGgH4DB6cH8+rNe7V1X5+65rdkp2zczvdOHcpE47h8D/d89TFhBAlhBTVAVKhRARA7TvUp1s3+zGod68aAfjhNg5jzmKJXr6mkPb3Hs43jTBGudbyrN+/NGbfb9FWhS5DdjvP6nPwEP073y8aFSCoyKgAC5fWbv/03/UL2/Cn0moWc54Hdvbrjkf3KSGqor9HMxnTOKh6z07PbuK33JXm327ce69by37ptgGFWKRWLLAvihowKgEh4FfHm608iFVckWup5Vixu07ULJopmpdSk5nL2Alzr7s2m8Vu+dvtOY7UW4lrHby/Qdeu0W0iWhFVCSCoyKgBKZs0SSCook2GtTym2riOITRmdNj8sthak0BoWr52j7f1g/FwfSAKKaQGUTakPTK/jC9312S3oyTfGoKac/FwrKIWOmdVAiBOmfoAqE2WxZKnTCtZmb3ZO0zrWXYvtBa4/e74v24DNvNc+PePEqTGd/RpOBodHtXHbgZz3uBWz+jlfIdymt9z+LbD3D5IoFoHKPffco/PPP19Tp07VZZddpueeey7qIQGJE+VDqNTmY9YVOnZOD33rrsUbtx3I2SDQniK27svjp0Ou0zXc6kO6t+x3fI/T55HvfEFy+7dAnQqSKPJA5aGHHtJtt92m7u5u/eY3v9GCBQu0bNkyvfnmm1EPDUiUJD+EvMbu9NA33WSbG+o1PDqezaQYtamJmg7JPVuTL7CzXsNpXJt2HszubeS1e7Lf8xUiX5Dl9nnSzRZJFHmNymWXXaYPfehD+pd/+RdJ0vj4uFpbW3XLLbdo7dq1eY+nRgWIRlBLi0u1cN32bOt6p0JYtw61Um7RbxAFsNYmc3ffuCi0ew6j3gYot0TUqJw5c0Z79uzR1VdfnX2tpqZGV199tZ555hnHY4aHhzUwMJDzBaD8nDISUUw/rVnWkc1UrFnW4Zh9sQYpbnvrWDcXtGcr/C5/Nh11t+57J7sThkKWQAeFhnGISqSByh//+EeNjY1p9uzZOa/Pnj1bR44ccTxm/fr1ampqyn61traWY6gAbLw26Cvn9NOKxW3q6V6qnu6l2cyI2+Z99vE57V4sadJD3u99WTcwDIMZryTPKRyvDRvdzpkvAKEQF1GJdOqnr69P5513np5++mldfvnl2de/9rWv6cknn9Szzz476Zjh4WENDw9n/z4wMKDW1lamfoAQxXkqwd5rxDxQ/SwNNtNGzQ316ulemj2ffXqomD4vQfWIsSpm2XO+Y/yeM87/BpBMiZj6efe7363a2lq98cYbOa+/8cYbmjNnjuMx6XRa06dPz/kCEK44/zZtX01TalbHmoEx9711X1/B92+OffT5wo91U8wePvk+D1PkOzg86plVoRAXUYk0UJkyZYo6Ozu1Y8eO7Gvj4+PasWNHToYFQLTCmtIJou7BvpqmkAfqmmUdOq+5IbtCyOncTi30/Y5roiW/vw0W/XwWZvpq086DrhskOh3j9nmYLImksiybBooR+aqfhx56SCtXrtS9996rSy+9VN///vf105/+VC+99NKk2hUnrPoBkqvcLeDztaQv9dylrILy+1n42djR71gHh0ezU19+tw0AguL3+V1XxjE5+su//EsdPXpU3/zmN3XkyBEtXLhQP//5z30FKQCSxf5QtS8XDuL8XoGImSaSpI3bDgT6ULavKJImshl+r+H3s7C/r5Br2Mfa3FCfzRQRoCCuIs+olIqMChBPTr/p+8kalLLBoDm/JNd9g+54ZL8ykuprUhrPZHRxS5PeGjxT8oaIfopmoyxIDapIGAhKIoppAVQup9oJP7Uufgp3vVrEe3V/XbG4TdcumFhCPDKe0VhG2nf4REnFrvYW/ZJca02KKUo2tSurN+91PG8xy4tN3cqe3uOxLZIGDAIVAKFwCkr8FLr6CWa8WsRbe6o4MfsKNdTXqjYlzT+vyXehrFNQYB+LVzBSTFFyvpVHfoOfuPS9yYfGcrBj6gdARXCa2sjXDt/+vnxTM6VOXRVb+Lpx2wENj44pXVc7qfYmLlsZBKXcBdaIDlM/ABKp2N+orZkFryyDU58U8z4/GxWe19ygzrYZrmP0yhoVM/WzYnGbGtN1GhoZV2O6btJ53XZqtl7H/pnGOWsRxywPokWgAiBWim0uZ33AuT3s7A9o+/vyPSTttR1O+wL5HaPTeNwCiEIf3vmmo+LcwI/GcrBj6gdArIQ5bRHUtIK9D4n9fH77tdjHY/17Z9uMwHZitn+mSZ4aQuVg6gcAbPxkJvx2iH1q7VXZzrb2lvbdW/bntPX3Ox7r34PcidmepSgka1HqNFGcp5mQDGRUAMRK1MWU5vq1KWnd9ZcUnHEwx6ckNTXUZ9vzF5rBWL1576SMShSZkFJ/HlH/PBFfZFQAJFLUxZRmj56xjIqq4TDj//YNl2SXSRdTE3L3jYt0cH1XzrRP2LUlfpZfFyrqnyeSj4wKAMj/8uZSz+13usXp/WFnVMh+oJzIqABIFKff5v2+FgSnzq1BBQPmfJJ711q3sRhBBilBrS4CyoFABUAsOD2c/b4WhHI8pM3Y73hkvxau2+4asDgtY+7esj+w+3b7DFkajDgiUAEQC35bvIcVUJTjIW3qXzKS54og+1g27TyosYxUm1Le+/aTcSJzgiShRgVA1YhD/xC/PVbsx/gdN3UmSAq/z++6Mo4JACJlr0MJk9cePH4DFGPF4jbf7+9sm6EjJ4bU2TajqHEDccPUD4Cq4TXlkW/KZPXmvWq/fatWb97r61rlrK+xMrtD7+k9Hto1gHIiUAFQNUrZMLDQTrFh1dfkC6ioP0GloUYFQCyVu54k3/WcOsVGoZpqUOJQU4Tw+H1+E6gAiKViH8iV/nCr9PuzqqagrBrR8A1AohU7hVGOOpAolXNDwajPzzQWJDIqACpMuTIOSchslJKR8Lo/873B4VH1D40UnfFIwmeI8JBRAVCVytVd1StzE3amwS9rRqLQMXndn/mepJIyHpWe/UIwCFQAoAhe0xJxeQBbgza/YzIBTWfbDNf7M/e+ZllHTlBYaDDE1A78oOEbABTBqwnbqiXtOTsxx4HTmJymXjZuO6D+oRENDo+qp3up47msrf3tfy+koV4hjexQvcioAICFPStQzDROsdNPYU4ZOY2plMyP07FBZ0jiMoWGaBGoAICF/QFczmmcck8ZOQUWa5Z1ZKd17KyBg9OxQdcHxWUKDdFi1Q8AWNinQ4JemeJ1fkmxXgVT7r4mrAqqbDR8A4AYsj/sk9TUjMABQWJ5MgDEkH3J8ODwqJob6kMtvA2q1iPf1A41JQgDgQoA+BDGw37TzoPqHxpRY7ou1AxFuWo9qClBGAhUAMCHMB7ChaySKSVQKle/EvqiIAzUqACAB1OX0dk2Q3t6j0dWn2GvZYlLvUhcxoHkoUYFAAJgMil7eo+XpTW/m1VL2tXcUK/B4dFscBCHaZYkbCWAZCNQAZAo5X74xWU6wwRI/UMj2rjtQGzGlYStBJBsBCoAEiWMh59X8FOuTQ4LFca43D6HYj+fuARTSDYCFQCJEsbDr5y/+ZeSEfLqGhsEt8+h2M8nrkEekoVABUCihPHwK+dv/m4PfT/ZjLD3ELL3eLliwy+0evPesvR6Adyw6gcAyshtlYxbh9p8nWv9rLoppvutOaY2JY1llIjOuUgWVv0AQAy5ZUXcsjr5sj1+pmWKyRiZY7rmt1BngkiRUQGABCtnHxN6piBIbEoIAAhUkjZQRPwx9QMACBTLjREFMioAUGWYwkEckFEBADiiYyyShEAFAKoMUzhIEqZ+AABA2TH1AwAAEi/SQOX8889XKpXK+dqwYUOUQwKAqlDInkPl3rEasIo8o/Ktb31Lr7/+evbrlltuiXpIAFDxCimopfgWUYo8UJk2bZrmzJmT/WpsbIx6SABQ8QopqC2m+JYsDIISaTHt+eefr9OnT2tkZERz587VX/3VX+nLX/6y6urqXI8ZHh7W8PBw9u8DAwNqbW2lmBYAYoQutsgnEcW0q1ev1oMPPqgnnnhCn//85/Wd73xHX/va1zyPWb9+vZqamrJfra2tZRotAMCvpC+BJiMUH4FnVNauXavvfve7nu/53e9+p/e///2TXv/P//xPff7zn9epU6eUTqcdjyWjAgAIGxmh8PnNqLjPsRTpK1/5im666SbP91xwwQWOr1922WUaHR3VH/7wB3V0dDi+J51OuwYxAAAEYdWS9uw2A4hW4IHKrFmzNGvWrKKO7enpUU1Njc4555yARwUAgH8rFrexD1JMBB6o+PXMM8/o2Wef1ZVXXqlp06bpmWee0Ze//GWtWLFCM2bMiGpYAFC12KwQcRRZMW06ndaDDz6oj33sY7r44ot155136stf/rL+9V//NaohAUBVo18K4iiyjMqf/MmfaPfu3VFdHgBgQ10G4ohNCQEAQNkloo8KAACAFwIVAEBBaIaGciJQAQAUhKJblBOBCgCgIElvj49koZgWAFDV6B8TDYppAQDwgamseCNQAQBUNaay4o2pHwAAUHZM/QAAgMQjUAEAALFFoAIAiDUazFW3yDYlBADAD/uqHJYSVxcyKgCAWLOuymEpcfUhowIAiLUVi9tysicmo4LqwPJkAABQdixPBgAAiUegAgAAYotABQAAxBaBCgAAMUT/mAkEKgAAxBBLsScQqAAAEEPs6jyB5ckAAKDsWJ4MAAASj0AFAADEFoEKAACILQIVAAAQWwQqAABX9PJA1AhUAACu6OWBqBGoAABcOfXyIMuCcqKPCgCgIFds+IUO9w/pvOYGPbX2qqiHg4SijwoAIBR0TEU5kVEBAABlR0YFAAAkHoEKAACILQIVAAAQWwQqAAAgtghUAABAbBGoAACA2CJQAQAAsUWgAgAAYotABQAAxBaBCgAAiC0CFQAAEFsEKgAAILYIVAAAQGwRqAAAgNgKLVC588479eEPf1hnnXWWmpubHd9z6NAhdXV16ayzztI555yjr371qxodHQ1rSAAAIGHqwjrxmTNn9IlPfEKXX365/uM//mPS98fGxtTV1aU5c+bo6aef1uuvv67PfOYzqq+v13e+852whgUAABIklclkMmFe4P7779ett96q/v7+nNcfe+wx/dmf/Zn6+vo0e/ZsSdIPf/hDff3rX9fRo0c1ZcoUX+cfGBhQU1OTTpw4oenTpwc9fAAAEAK/z+/IalSeeeYZffCDH8wGKZK0bNkyDQwM6MUXX3Q9bnh4WAMDAzlfAACgMkUWqBw5ciQnSJGU/fuRI0dcj1u/fr2ampqyX62traGOEwAARKegQGXt2rVKpVKeXy+99FJYY5Uk3X777Tpx4kT269VXXw31egAAIDoFFdN+5Stf0U033eT5ngsuuMDXuebMmaPnnnsu57U33ngj+z036XRa6XTa1zUAAECyFRSozJo1S7NmzQrkwpdffrnuvPNOvfnmmzrnnHMkSY8//rimT5+uiy66KJBrAACAZAttefKhQ4d07NgxHTp0SGNjY+rp6ZEkvfe979W73vUuLV26VBdddJE+/elP66677tKRI0f0jW98QzfffDMZEwAAICnE5ck33XST/uu//mvS60888YSWLFkiSert7dWqVau0c+dONTY2auXKldqwYYPq6vzHTyxPBgAgefw+v0PvoxI2AhUAAJIn9n1UAAAA8iFQAQAAsUWgAgAAYotABQAAxBaBCgAAiC0CFQAAEFsEKgAAILYIVAAAQGwRqAAAgNgiUAEAALFFoAIAAGKLQAUAAMQWgQoAAIgtAhUAABBbBCoAACC2CFQAAEBsEagAAIDYIlABAACxRaACAABii0AFAADEFoEKAACILQIVAADg6IHdvbpiwy/0wO7eyMZAoAIAABxt2nlQh/uHtGnnwcjGQKACAAAcrVrSrvOaG7RqSXtkY0hlMplMZFcPwMDAgJqamnTixAlNnz496uEAAAAf/D6/yagAAIDYIlABAACxRaACAABii0AFAADEFoEKAACILQIVAAAQWwQqAAAgtghUAABAbBGoAACA2CJQAQAAsUWgAgAAYotABQAAxBaBCgAAiK26qAdQKrP588DAQMQjAQAAfpnntnmOu0l8oHLy5ElJUmtra8QjAQAAhTp58qSamppcv5/K5AtlYm58fFx9fX2aNm2aUqlU2a8/MDCg1tZWvfrqq5o+fXrZrx+1ar5/7r06712q7vuv5nuXqvv+g773TCajkydPqqWlRTU17pUoic+o1NTU6D3veU/Uw9D06dOr7h+tVTXfP/denfcuVff9V/O9S9V9/0Heu1cmxaCYFgAAxBaBCgAAiC0ClRKl02l1d3crnU5HPZRIVPP9c+/Vee9Sdd9/Nd+7VN33H9W9J76YFgAAVC4yKgAAILYIVAAAQGwRqAAAgNgiUAEAALFFoBKw6667TnPnztXUqVN17rnn6tOf/rT6+vqiHlbo/vCHP+hzn/uc5s2bp4aGBrW3t6u7u1tnzpyJemhlceedd+rDH/6wzjrrLDU3N0c9nNDdc889Ov/88zV16lRddtlleu6556IeUlns2rVL1157rVpaWpRKpfTII49EPaSyWb9+vT70oQ9p2rRpOuecc3TDDTfowIEDUQ+rLDZt2qT58+dnG51dfvnleuyxx6IeViQ2bNigVCqlW2+9tWzXJFAJ2JVXXqmf/vSnOnDggP7nf/5HBw8e1F/8xV9EPazQvfTSSxofH9e9996rF198Uf/8z/+sH/7wh/q7v/u7qIdWFmfOnNEnPvEJrVq1KuqhhO6hhx7Sbbfdpu7ubv3mN7/RggULtGzZMr355ptRDy10g4ODWrBgge65556oh1J2Tz75pG6++Wbt3r1bjz/+uEZGRrR06VINDg5GPbTQvec979GGDRu0Z88e/frXv9ZVV12l66+/Xi+++GLUQyurX/3qV7r33ns1f/788l44g1Bt2bIlk0qlMmfOnIl6KGV31113ZebNmxf1MMrqvvvuyzQ1NUU9jFBdeumlmZtvvjn797GxsUxLS0tm/fr1EY6q/CRlHn744aiHEZk333wzIynz5JNPRj2USMyYMSPz7//+71EPo2xOnjyZed/73pd5/PHHMx/72McyX/rSl8p2bTIqITp27Jh+/OMf68Mf/rDq6+ujHk7ZnThxQjNnzox6GAjQmTNntGfPHl199dXZ12pqanT11VfrmWeeiXBkKLcTJ05IUtX9b3xsbEwPPvigBgcHdfnll0c9nLK5+eab1dXVlfO//XIhUAnB17/+dTU2Nurss8/WoUOHtGXLlqiHVHYvv/yyfvCDH+jzn/981ENBgP74xz9qbGxMs2fPznl99uzZOnLkSESjQrmNj4/r1ltv1RVXXKFLLrkk6uGUxQsvvKB3vetdSqfT+sIXvqCHH35YF110UdTDKosHH3xQv/nNb7R+/fpIrk+g4sPatWuVSqU8v1566aXs+7/61a9q79692r59u2pra/WZz3xGmYQ2AC703iXp8OHDuuaaa/SJT3xCf/M3fxPRyEtXzL0D1eDmm2/W/v379eCDD0Y9lLLp6OhQT0+Pnn32Wa1atUorV67Ub3/726iHFbpXX31VX/rSl/TjH/9YU6dOjWQMtND34ejRo3rrrbc833PBBRdoypQpk15/7bXX1NraqqeffjqRacJC772vr09LlizR4sWLdf/996umJrmxcDE/9/vvv1+33nqr+vv7Qx5dNM6cOaOzzjpL//3f/60bbrgh+/rKlSvV399fVdnDVCqlhx9+OOdzqAZf/OIXtWXLFu3atUvz5s2LejiRufrqq9Xe3q5777036qGE6pFHHtGf//mfq7a2Nvva2NiYUqmUampqNDw8nPO9MNSFevYKMWvWLM2aNauoY8fHxyVJw8PDQQ6pbAq598OHD+vKK69UZ2en7rvvvkQHKVJpP/dKNWXKFHV2dmrHjh3ZB/T4+Lh27NihL37xi9EODqHKZDK65ZZb9PDDD2vnzp1VHaRIE//uk/r/64X4+Mc/rhdeeCHntc9+9rN6//vfr69//euhBykSgUqgnn32Wf3qV7/SRz7yEc2YMUMHDx7UHXfcofb29kRmUwpx+PBhLVmyRG1tbdq4caOOHj2a/d6cOXMiHFl5HDp0SMeOHdOhQ4c0Njamnp4eSdJ73/tevetd74p2cAG77bbbtHLlSv3pn/6pLr30Un3/+9/X4OCgPvvZz0Y9tNCdOnVKL7/8cvbvr7zyinp6ejRz5kzNnTs3wpGF7+abb9ZPfvITbdmyRdOmTcvWJDU1NamhoSHi0YXr9ttv1/LlyzV37lydPHlSP/nJT7Rz505t27Yt6qGFbtq0aZPqkEwNZtnqk8q2vqgK7Nu3L3PllVdmZs6cmUmn05nzzz8/84UvfCHz2muvRT200N13330ZSY5f1WDlypWO9/7EE09EPbRQ/OAHP8jMnTs3M2XKlMyll16a2b17d9RDKosnnnjC8ee8cuXKqIcWOrf/fd93331RDy10f/3Xf51pa2vLTJkyJTNr1qzMxz/+8cz27dujHlZkyr08mRoVAAAQW8kuIgAAABWNQAUAAMQWgQoAAIgtAhUAABBbBCoAACC2CFQAAEBsEagAAIDYIlABAACxRaACAABii0AFAADEFoEKAACILQIVAAAQW/8f8aC/wIbACCcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出第二个特征与真实值的散点图\n",
    "plt.scatter(features[:, (1)], labels, 1); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MindSpore的GeneratorDataset创建可迭代数据。<br>\n",
    "dataset模块提供了加载和处理数据集的API；numpy提供了一系列类NumPy接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "from mindspore import dataset as ds  \n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.data = features\n",
    "        self.label = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "batch_size = 10\n",
    "dataset_generator = DatasetGenerator()\n",
    "dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"label\"], shuffle=True)\n",
    "# 将数据集中连续10条数据合并为一个批处理数据\n",
    "dataset = dataset.batch(batch_size)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建分为定义实现二维线性回归功能的Net、用于计算预测值与标签值之间的均方误差MSELoss、Adam优化器。\n",
    "- 定义模型Net\n",
    "\n",
    "Net输入为X样本，计算出预测值$\\widehat y$并返回。<br>\n",
    "mindspore.nn用于构建神经网络中的预定义构建块或计算单元；Parameter 是 Tensor 的子类，当它们被绑定为Cell的属性时，会自动添加到其参数列表中，并且可以通过Cell的某些方法获取；mindspore.common.initializer用于初始化神经元参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import initializer, Zero, Normal\n",
    "\n",
    "\n",
    "def linreg(x, w, b):\n",
    "    # y = Xw+b\n",
    "    return ops.matmul(x, w) + b    \n",
    "\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = Parameter(initializer(Normal(0.01, 0), (2, 1), mstype.float32))\n",
    "        self.b = Parameter(initializer(Zero(), 1, mstype.float32))\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # y_hat = Xw+b\n",
    "        y_hat = linreg(x, self.w, self.b)  \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "# Net用于实现二维线性回归\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用MSE损失函数和Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练的eopch为3\n",
    "num_epochs = 3\n",
    "# 学习率为0.03\n",
    "lr = 0.03\n",
    "# Adam优化器\n",
    "optim = nn.Adam(net.trainable_params(), learning_rate=lr)          \n",
    "# 计算预测值与标签值之间的均方误差\n",
    "loss = nn.MSELoss()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、模型训练\n",
    "Model是模型训练与推理的高阶接口，调用Model.train方法，传入数据集即可完成模型训练，其中LossMonitor()会监控训练的损失，并将epoch、step、loss信息打印出来，若loss变为NAN或INF，则会终止训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.train import Model                                       \n",
    "from mindspore.train import LossMonitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 29.33513832092285\n",
      "epoch: 1 step: 2, loss is 37.46299743652344\n",
      "epoch: 1 step: 3, loss is 39.28642272949219\n",
      "epoch: 1 step: 4, loss is 41.847991943359375\n",
      "epoch: 1 step: 5, loss is 16.808752059936523\n",
      "epoch: 1 step: 6, loss is 42.482215881347656\n",
      "epoch: 1 step: 7, loss is 29.145341873168945\n",
      "epoch: 1 step: 8, loss is 33.21084213256836\n",
      "epoch: 1 step: 9, loss is 30.48308563232422\n",
      "epoch: 1 step: 10, loss is 27.18667221069336\n",
      "epoch: 1 step: 11, loss is 41.479427337646484\n",
      "epoch: 1 step: 12, loss is 50.3975715637207\n",
      "epoch: 1 step: 13, loss is 17.694313049316406\n",
      "epoch: 1 step: 14, loss is 35.32475280761719\n",
      "epoch: 1 step: 15, loss is 51.93513870239258\n",
      "epoch: 1 step: 16, loss is 18.89722442626953\n",
      "epoch: 1 step: 17, loss is 30.43497657775879\n",
      "epoch: 1 step: 18, loss is 25.525924682617188\n",
      "epoch: 1 step: 19, loss is 30.582584381103516\n",
      "epoch: 1 step: 20, loss is 12.64999771118164\n",
      "epoch: 1 step: 21, loss is 31.38129425048828\n",
      "epoch: 1 step: 22, loss is 19.548561096191406\n",
      "epoch: 1 step: 23, loss is 13.861833572387695\n",
      "epoch: 1 step: 24, loss is 21.32258415222168\n",
      "epoch: 1 step: 25, loss is 17.00946807861328\n",
      "epoch: 1 step: 26, loss is 13.774389266967773\n",
      "epoch: 1 step: 27, loss is 33.077484130859375\n",
      "epoch: 1 step: 28, loss is 13.877609252929688\n",
      "epoch: 1 step: 29, loss is 16.375003814697266\n",
      "epoch: 1 step: 30, loss is 12.307830810546875\n",
      "epoch: 1 step: 31, loss is 12.469795227050781\n",
      "epoch: 1 step: 32, loss is 15.963699340820312\n",
      "epoch: 1 step: 33, loss is 19.221111297607422\n",
      "epoch: 1 step: 34, loss is 17.515111923217773\n",
      "epoch: 1 step: 35, loss is 10.54263973236084\n",
      "epoch: 1 step: 36, loss is 16.58633041381836\n",
      "epoch: 1 step: 37, loss is 14.373174667358398\n",
      "epoch: 1 step: 38, loss is 14.532005310058594\n",
      "epoch: 1 step: 39, loss is 7.7050981521606445\n",
      "epoch: 1 step: 40, loss is 13.156425476074219\n",
      "epoch: 1 step: 41, loss is 32.52201461791992\n",
      "epoch: 1 step: 42, loss is 19.205060958862305\n",
      "epoch: 1 step: 43, loss is 26.562786102294922\n",
      "epoch: 1 step: 44, loss is 29.958065032958984\n",
      "epoch: 1 step: 45, loss is 16.398605346679688\n",
      "epoch: 1 step: 46, loss is 23.833070755004883\n",
      "epoch: 1 step: 47, loss is 13.132551193237305\n",
      "epoch: 1 step: 48, loss is 13.920884132385254\n",
      "epoch: 1 step: 49, loss is 19.103349685668945\n",
      "epoch: 1 step: 50, loss is 15.145254135131836\n",
      "epoch: 1 step: 51, loss is 16.269298553466797\n",
      "epoch: 1 step: 52, loss is 10.03077220916748\n",
      "epoch: 1 step: 53, loss is 16.784996032714844\n",
      "epoch: 1 step: 54, loss is 9.908897399902344\n",
      "epoch: 1 step: 55, loss is 15.838116645812988\n",
      "epoch: 1 step: 56, loss is 13.97175407409668\n",
      "epoch: 1 step: 57, loss is 15.69272232055664\n",
      "epoch: 1 step: 58, loss is 14.330774307250977\n",
      "epoch: 1 step: 59, loss is 13.212286949157715\n",
      "epoch: 1 step: 60, loss is 9.237480163574219\n",
      "epoch: 1 step: 61, loss is 14.53261947631836\n",
      "epoch: 1 step: 62, loss is 17.80704689025879\n",
      "epoch: 1 step: 63, loss is 4.714548110961914\n",
      "epoch: 1 step: 64, loss is 3.766012191772461\n",
      "epoch: 1 step: 65, loss is 8.403373718261719\n",
      "epoch: 1 step: 66, loss is 8.255990982055664\n",
      "epoch: 1 step: 67, loss is 4.484724521636963\n",
      "epoch: 1 step: 68, loss is 5.450994968414307\n",
      "epoch: 1 step: 69, loss is 7.779852867126465\n",
      "epoch: 1 step: 70, loss is 4.426551818847656\n",
      "epoch: 1 step: 71, loss is 12.461235046386719\n",
      "epoch: 1 step: 72, loss is 14.55686092376709\n",
      "epoch: 1 step: 73, loss is 0.805408239364624\n",
      "epoch: 1 step: 74, loss is 7.416049480438232\n",
      "epoch: 1 step: 75, loss is 8.820674896240234\n",
      "epoch: 1 step: 76, loss is 4.946131229400635\n",
      "epoch: 1 step: 77, loss is 6.882390975952148\n",
      "epoch: 1 step: 78, loss is 8.535741806030273\n",
      "epoch: 1 step: 79, loss is 7.40862512588501\n",
      "epoch: 1 step: 80, loss is 2.4266157150268555\n",
      "epoch: 1 step: 81, loss is 10.088254928588867\n",
      "epoch: 1 step: 82, loss is 6.668605804443359\n",
      "epoch: 1 step: 83, loss is 5.525445938110352\n",
      "epoch: 1 step: 84, loss is 3.769009828567505\n",
      "epoch: 1 step: 85, loss is 5.503881931304932\n",
      "epoch: 1 step: 86, loss is 9.117692947387695\n",
      "epoch: 1 step: 87, loss is 5.381252288818359\n",
      "epoch: 1 step: 88, loss is 6.088679313659668\n",
      "epoch: 1 step: 89, loss is 6.242405891418457\n",
      "epoch: 1 step: 90, loss is 7.096940040588379\n",
      "epoch: 1 step: 91, loss is 9.609804153442383\n",
      "epoch: 1 step: 92, loss is 7.569792747497559\n",
      "epoch: 1 step: 93, loss is 4.220124244689941\n",
      "epoch: 1 step: 94, loss is 6.169042587280273\n",
      "epoch: 1 step: 95, loss is 3.3124465942382812\n",
      "epoch: 1 step: 96, loss is 7.952907085418701\n",
      "epoch: 1 step: 97, loss is 8.146528244018555\n",
      "epoch: 1 step: 98, loss is 10.936038970947266\n",
      "epoch: 1 step: 99, loss is 4.389820098876953\n",
      "epoch: 1 step: 100, loss is 6.11431884765625\n",
      "epoch: 2 step: 1, loss is 4.24835729598999\n",
      "epoch: 2 step: 2, loss is 6.954296112060547\n",
      "epoch: 2 step: 3, loss is 9.254989624023438\n",
      "epoch: 2 step: 4, loss is 6.720329284667969\n",
      "epoch: 2 step: 5, loss is 4.685884475708008\n",
      "epoch: 2 step: 6, loss is 5.312098026275635\n",
      "epoch: 2 step: 7, loss is 3.214857578277588\n",
      "epoch: 2 step: 8, loss is 4.708015441894531\n",
      "epoch: 2 step: 9, loss is 7.731549263000488\n",
      "epoch: 2 step: 10, loss is 2.787165641784668\n",
      "epoch: 2 step: 11, loss is 2.2660071849823\n",
      "epoch: 2 step: 12, loss is 2.5034518241882324\n",
      "epoch: 2 step: 13, loss is 3.5088019371032715\n",
      "epoch: 2 step: 14, loss is 5.122994422912598\n",
      "epoch: 2 step: 15, loss is 4.862705707550049\n",
      "epoch: 2 step: 16, loss is 3.035531520843506\n",
      "epoch: 2 step: 17, loss is 3.1029810905456543\n",
      "epoch: 2 step: 18, loss is 5.455187797546387\n",
      "epoch: 2 step: 19, loss is 1.901469111442566\n",
      "epoch: 2 step: 20, loss is 4.915130615234375\n",
      "epoch: 2 step: 21, loss is 2.6049745082855225\n",
      "epoch: 2 step: 22, loss is 2.415421724319458\n",
      "epoch: 2 step: 23, loss is 1.0339387655258179\n",
      "epoch: 2 step: 24, loss is 2.31264591217041\n",
      "epoch: 2 step: 25, loss is 0.879693865776062\n",
      "epoch: 2 step: 26, loss is 1.8836594820022583\n",
      "epoch: 2 step: 27, loss is 2.77068829536438\n",
      "epoch: 2 step: 28, loss is 4.186056613922119\n",
      "epoch: 2 step: 29, loss is 2.369800329208374\n",
      "epoch: 2 step: 30, loss is 2.2341272830963135\n",
      "epoch: 2 step: 31, loss is 2.423903226852417\n",
      "epoch: 2 step: 32, loss is 2.3111841678619385\n",
      "epoch: 2 step: 33, loss is 1.8182461261749268\n",
      "epoch: 2 step: 34, loss is 4.634824752807617\n",
      "epoch: 2 step: 35, loss is 2.4480247497558594\n",
      "epoch: 2 step: 36, loss is 1.6808935403823853\n",
      "epoch: 2 step: 37, loss is 2.1961750984191895\n",
      "epoch: 2 step: 38, loss is 1.9911036491394043\n",
      "epoch: 2 step: 39, loss is 2.2821977138519287\n",
      "epoch: 2 step: 40, loss is 3.2957475185394287\n",
      "epoch: 2 step: 41, loss is 2.353273630142212\n",
      "epoch: 2 step: 42, loss is 1.7115195989608765\n",
      "epoch: 2 step: 43, loss is 1.6333167552947998\n",
      "epoch: 2 step: 44, loss is 1.0838332176208496\n",
      "epoch: 2 step: 45, loss is 1.5041457414627075\n",
      "epoch: 2 step: 46, loss is 1.4659168720245361\n",
      "epoch: 2 step: 47, loss is 1.9388890266418457\n",
      "epoch: 2 step: 48, loss is 2.3239495754241943\n",
      "epoch: 2 step: 49, loss is 1.3613390922546387\n",
      "epoch: 2 step: 50, loss is 1.0556316375732422\n",
      "epoch: 2 step: 51, loss is 0.8716793060302734\n",
      "epoch: 2 step: 52, loss is 1.1123721599578857\n",
      "epoch: 2 step: 53, loss is 1.1657922267913818\n",
      "epoch: 2 step: 54, loss is 1.0019348859786987\n",
      "epoch: 2 step: 55, loss is 1.7948863506317139\n",
      "epoch: 2 step: 56, loss is 0.9553043842315674\n",
      "epoch: 2 step: 57, loss is 0.6728433966636658\n",
      "epoch: 2 step: 58, loss is 0.8953102827072144\n",
      "epoch: 2 step: 59, loss is 0.8105589151382446\n",
      "epoch: 2 step: 60, loss is 0.9845868945121765\n",
      "epoch: 2 step: 61, loss is 1.4264308214187622\n",
      "epoch: 2 step: 62, loss is 0.939066469669342\n",
      "epoch: 2 step: 63, loss is 1.0670902729034424\n",
      "epoch: 2 step: 64, loss is 0.6336681842803955\n",
      "epoch: 2 step: 65, loss is 0.8376425504684448\n",
      "epoch: 2 step: 66, loss is 0.39321818947792053\n",
      "epoch: 2 step: 67, loss is 0.8112329840660095\n",
      "epoch: 2 step: 68, loss is 0.4563824534416199\n",
      "epoch: 2 step: 69, loss is 1.1441807746887207\n",
      "epoch: 2 step: 70, loss is 1.497287392616272\n",
      "epoch: 2 step: 71, loss is 0.5989107489585876\n",
      "epoch: 2 step: 72, loss is 0.7809985876083374\n",
      "epoch: 2 step: 73, loss is 0.5138855576515198\n",
      "epoch: 2 step: 74, loss is 0.5772721767425537\n",
      "epoch: 2 step: 75, loss is 0.3831925392150879\n",
      "epoch: 2 step: 76, loss is 0.5262008905410767\n",
      "epoch: 2 step: 77, loss is 1.1277823448181152\n",
      "epoch: 2 step: 78, loss is 0.46366259455680847\n",
      "epoch: 2 step: 79, loss is 0.3736445903778076\n",
      "epoch: 2 step: 80, loss is 0.3862391710281372\n",
      "epoch: 2 step: 81, loss is 0.7016186714172363\n",
      "epoch: 2 step: 82, loss is 0.7563797831535339\n",
      "epoch: 2 step: 83, loss is 0.5322238206863403\n",
      "epoch: 2 step: 84, loss is 0.5376900434494019\n",
      "epoch: 2 step: 85, loss is 0.791725754737854\n",
      "epoch: 2 step: 86, loss is 0.22769233584403992\n",
      "epoch: 2 step: 87, loss is 0.18572819232940674\n",
      "epoch: 2 step: 88, loss is 0.36855876445770264\n",
      "epoch: 2 step: 89, loss is 0.8092793226242065\n",
      "epoch: 2 step: 90, loss is 0.6981639862060547\n",
      "epoch: 2 step: 91, loss is 0.5612894296646118\n",
      "epoch: 2 step: 92, loss is 0.38392332196235657\n",
      "epoch: 2 step: 93, loss is 0.2640663981437683\n",
      "epoch: 2 step: 94, loss is 0.21307089924812317\n",
      "epoch: 2 step: 95, loss is 0.445789098739624\n",
      "epoch: 2 step: 96, loss is 0.4479224979877472\n",
      "epoch: 2 step: 97, loss is 0.5040189027786255\n",
      "epoch: 2 step: 98, loss is 0.41459375619888306\n",
      "epoch: 2 step: 99, loss is 0.24770595133304596\n",
      "epoch: 2 step: 100, loss is 0.45965832471847534\n",
      "epoch: 3 step: 1, loss is 0.2981744110584259\n",
      "epoch: 3 step: 2, loss is 0.2553741931915283\n",
      "epoch: 3 step: 3, loss is 0.4366358816623688\n",
      "epoch: 3 step: 4, loss is 0.3082890212535858\n",
      "epoch: 3 step: 5, loss is 0.379729688167572\n",
      "epoch: 3 step: 6, loss is 0.3965895175933838\n",
      "epoch: 3 step: 7, loss is 0.2743699848651886\n",
      "epoch: 3 step: 8, loss is 0.1107001006603241\n",
      "epoch: 3 step: 9, loss is 0.1342698186635971\n",
      "epoch: 3 step: 10, loss is 0.15186041593551636\n",
      "epoch: 3 step: 11, loss is 0.17861773073673248\n",
      "epoch: 3 step: 12, loss is 0.23659630119800568\n",
      "epoch: 3 step: 13, loss is 0.19689032435417175\n",
      "epoch: 3 step: 14, loss is 0.195046529173851\n",
      "epoch: 3 step: 15, loss is 0.155483677983284\n",
      "epoch: 3 step: 16, loss is 0.13549888134002686\n",
      "epoch: 3 step: 17, loss is 0.2813482880592346\n",
      "epoch: 3 step: 18, loss is 0.21725647151470184\n",
      "epoch: 3 step: 19, loss is 0.19826796650886536\n",
      "epoch: 3 step: 20, loss is 0.17038053274154663\n",
      "epoch: 3 step: 21, loss is 0.26234978437423706\n",
      "epoch: 3 step: 22, loss is 0.2266811728477478\n",
      "epoch: 3 step: 23, loss is 0.264788955450058\n",
      "epoch: 3 step: 24, loss is 0.212098628282547\n",
      "epoch: 3 step: 25, loss is 0.14173269271850586\n",
      "epoch: 3 step: 26, loss is 0.10951755195856094\n",
      "epoch: 3 step: 27, loss is 0.30133339762687683\n",
      "epoch: 3 step: 28, loss is 0.07832592725753784\n",
      "epoch: 3 step: 29, loss is 0.21821048855781555\n",
      "epoch: 3 step: 30, loss is 0.19798272848129272\n",
      "epoch: 3 step: 31, loss is 0.20994475483894348\n",
      "epoch: 3 step: 32, loss is 0.1037198007106781\n",
      "epoch: 3 step: 33, loss is 0.1406460404396057\n",
      "epoch: 3 step: 34, loss is 0.2282170206308365\n",
      "epoch: 3 step: 35, loss is 0.049504008144140244\n",
      "epoch: 3 step: 36, loss is 0.04857566952705383\n",
      "epoch: 3 step: 37, loss is 0.07363783568143845\n",
      "epoch: 3 step: 38, loss is 0.08067646622657776\n",
      "epoch: 3 step: 39, loss is 0.14994938671588898\n",
      "epoch: 3 step: 40, loss is 0.10371281206607819\n",
      "epoch: 3 step: 41, loss is 0.12310857325792313\n",
      "epoch: 3 step: 42, loss is 0.09709793329238892\n",
      "epoch: 3 step: 43, loss is 0.05064721032977104\n",
      "epoch: 3 step: 44, loss is 0.10873649269342422\n",
      "epoch: 3 step: 45, loss is 0.06113532558083534\n",
      "epoch: 3 step: 46, loss is 0.049765847623348236\n",
      "epoch: 3 step: 47, loss is 0.08478738367557526\n",
      "epoch: 3 step: 48, loss is 0.09720957279205322\n",
      "epoch: 3 step: 49, loss is 0.11222808063030243\n",
      "epoch: 3 step: 50, loss is 0.06373581290245056\n",
      "epoch: 3 step: 51, loss is 0.08714865893125534\n",
      "epoch: 3 step: 52, loss is 0.024605438113212585\n",
      "epoch: 3 step: 53, loss is 0.04989315941929817\n",
      "epoch: 3 step: 54, loss is 0.05115659534931183\n",
      "epoch: 3 step: 55, loss is 0.044933900237083435\n",
      "epoch: 3 step: 56, loss is 0.0461658351123333\n",
      "epoch: 3 step: 57, loss is 0.06937642395496368\n",
      "epoch: 3 step: 58, loss is 0.07388422638177872\n",
      "epoch: 3 step: 59, loss is 0.09622713178396225\n",
      "epoch: 3 step: 60, loss is 0.07102786004543304\n",
      "epoch: 3 step: 61, loss is 0.05148264765739441\n",
      "epoch: 3 step: 62, loss is 0.02663017064332962\n",
      "epoch: 3 step: 63, loss is 0.02827340178191662\n",
      "epoch: 3 step: 64, loss is 0.050384145230054855\n",
      "epoch: 3 step: 65, loss is 0.06149779260158539\n",
      "epoch: 3 step: 66, loss is 0.03594039008021355\n",
      "epoch: 3 step: 67, loss is 0.04720902070403099\n",
      "epoch: 3 step: 68, loss is 0.025353528559207916\n",
      "epoch: 3 step: 69, loss is 0.03417685255408287\n",
      "epoch: 3 step: 70, loss is 0.029196040704846382\n",
      "epoch: 3 step: 71, loss is 0.03480830416083336\n",
      "epoch: 3 step: 72, loss is 0.034789107739925385\n",
      "epoch: 3 step: 73, loss is 0.027915263548493385\n",
      "epoch: 3 step: 74, loss is 0.041326720267534256\n",
      "epoch: 3 step: 75, loss is 0.011891552247107029\n",
      "epoch: 3 step: 76, loss is 0.03368235379457474\n",
      "epoch: 3 step: 77, loss is 0.04417184367775917\n",
      "epoch: 3 step: 78, loss is 0.03417826443910599\n",
      "epoch: 3 step: 79, loss is 0.023118838667869568\n",
      "epoch: 3 step: 80, loss is 0.020787658169865608\n",
      "epoch: 3 step: 81, loss is 0.023024413734674454\n",
      "epoch: 3 step: 82, loss is 0.04077079892158508\n",
      "epoch: 3 step: 83, loss is 0.03122526779770851\n",
      "epoch: 3 step: 84, loss is 0.010885979980230331\n",
      "epoch: 3 step: 85, loss is 0.011435924097895622\n",
      "epoch: 3 step: 86, loss is 0.028659330680966377\n",
      "epoch: 3 step: 87, loss is 0.01705290749669075\n",
      "epoch: 3 step: 88, loss is 0.03209582716226578\n",
      "epoch: 3 step: 89, loss is 0.023234937340021133\n",
      "epoch: 3 step: 90, loss is 0.02477288991212845\n",
      "epoch: 3 step: 91, loss is 0.019068893045186996\n",
      "epoch: 3 step: 92, loss is 0.02086888998746872\n",
      "epoch: 3 step: 93, loss is 0.020628206431865692\n",
      "epoch: 3 step: 94, loss is 0.011444364674389362\n",
      "epoch: 3 step: 95, loss is 0.011989878490567207\n",
      "epoch: 3 step: 96, loss is 0.013768801465630531\n",
      "epoch: 3 step: 97, loss is 0.012814630754292011\n",
      "epoch: 3 step: 98, loss is 0.025742044672369957\n",
      "epoch: 3 step: 99, loss is 0.009839225560426712\n",
      "epoch: 3 step: 100, loss is 0.012553527019917965\n"
     ]
    }
   ],
   "source": [
    "# 模型训练或推理的高阶接口。Model 会根据用户传入的参数封装可训练或推理的实例\n",
    "model = Model(net, loss_fn=loss, optimizer=optim)  \n",
    "# 模型训练接口。训练场景下，LossMonitor监控训练的loss；边训练边推理场景下，监控训练的loss和推理的metrics。如果loss是NAN或INF，则终止训练\n",
    "model.train(num_epochs, dataset, callbacks=[LossMonitor()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、模型预测\n",
    "训练3个epoch后输出w和b的估计误差，比较真实参数和通过训练学到的参数来评估训练的成功程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: [Tensor(shape=[], dtype=Float32, value= -0.0121365)\n",
      " Tensor(shape=[], dtype=Float32, value= -0.0563698)]\n",
      "b的估计误差: [Tensor(shape=[], dtype=Float32, value= 0.0883765)]\n"
     ]
    }
   ],
   "source": [
    "# w的真实值和训练值之差\n",
    "print(f'w的估计误差: {true_w - net.trainable_params()[0].reshape(true_w.shape)}')  \n",
    "# b的真实值和训练值之差\n",
    "print(f'b的估计误差: {true_b - net.trainable_params()[1]}')                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
