{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2f9a6da",
   "metadata": {},
   "source": [
    "# 26.基于MindSpore实现AC算法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0c388f6",
   "metadata": {},
   "source": [
    "本实验主要介绍使用MindSpore实现(AC)Actor-Critic算法，并使用自定义数据进行测试。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2485be66",
   "metadata": {},
   "source": [
    "# 1.实验目的\n",
    "- 理解AC算法原理。\n",
    "- 理解Actor和Critic部分的具体工作流程。\n",
    "- 自定义数据进行训练，根据奖励结果判断学习效果。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "176b810c",
   "metadata": {},
   "source": [
    "# 2. 算法知识点介绍\n",
    "Actor-Critic 算法合并了以策略为基础的 Policy Gradient和以值为基础的 Q-Learning 两类强化学习算法，该算法中将前者当作 Actor，用来基于概率选择行为。将后者当作 Critic，用来评判 Actor 的行为得分，然后 Actor 又会根据 Critic 的评分修改行为的概率。这使得它既可以在有效的处理连续动作的选取，又可以进行单步更新（PG算法的回合更新降低了学习效率）。\n",
    "\n",
    "下面分别介绍一下 Actor 网络和 Critic 网络这个两个部分。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b173cee0",
   "metadata": {},
   "source": [
    "## 2.1 Actor网络\n",
    "Actor网络采用的是基于策略的Policy-Grandient算法。PG算法的损失函数可以表示为 $loss = -E[\\log [\\pi (a|s)]\\cdot \\psi]$, 其中 $\\psi$ 是对某个轨迹的评分，在AC算法中它可以有多种表示方式：  \n",
    "1. 状态价值函数：$V(s)$  \n",
    "2. 动作价值函数：$Q(s,a)$\n",
    "3. TD-error: $r+\\gamma \\cdot Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)$\n",
    "4. 优势函数：$V(s,a)$\n",
    "5. 总回报：$\\sum r_t$\n",
    "6. 加入基线的总回报：$\\sum r_t -b$  \n",
    "  \n",
    "根据梯度策略算法的定义：策略优化目标函数如以下公式所示：\n",
    "$$\n",
    "L_{\\pi} = \\sum_{a \\in A}{\\log \\pi_{\\theta}(s_t, a_t)(G_{t}^{n} - V(s_t))}\n",
    "$$\n",
    "令 $advantage = G_{t}^{n} - V(s_t)$, 称 advantage 为优势函数。采用n步时序差分法求解时，$G_t$ 可以用公式表示如下：\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n V(s_{t+n})\n",
    "$$\n",
    "通过观察可以看出，当n为一个完整的状态序列大小时，该算法与蒙特卡洛算法等价。  \n",
    "    \n",
    "在实际编写代码时，采用TD-error作为评价，同时为了鼓励探索(exploration), 在损失函数中还加入了交叉熵损失。  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc577aa9",
   "metadata": {},
   "source": [
    "## 2.2 Critic网络\n",
    "Critic网络采用的是基于值函数的Q-Learning算法，采用 $loss = (TD_{error})^2$ 作为Critic网络的损失函数。  \n",
    "整个Actor-Critic算法流程如下图所示：  \n",
    "  \n",
    "![AC算法流程图](Figures/fig_001.png)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13d83f94",
   "metadata": {},
   "source": [
    "Actor-Critic算法流程：  \n",
    "准备：评价网络学习率 $\\alpha_1$ 和策略网络的学习率 $\\alpha_2$;\n",
    "1. 随机初始化评价网络参数 $\\theta_q$ 和策略网络参数 $\\theta_{\\pi}$;\n",
    "2. 重复以下操作至最大次数：\n",
    "3. 智能体与环境交互n步并收集由状态、动作、奖励构成的序列$\\{s_0, a_0, r_1, s_1, a_1, r_2,\\cdots \\}$;  \n",
    "4. 计算Critic网络的损失值 $L_i^{critic} = L_{critic}(s, r)$;  \n",
    "5. 计算Actor网络的损失值 $L_{actor}^i = L_{actor}(s, a, advantage)$;  \n",
    "6. 更新Critic网络参数 $\\theta_q \\leftarrow \\theta_q - \\alpha_1 \\nabla_{\\theta}\\sum_{i}L^{i}_{critic}$;    \n",
    "7. 更新Actor网络参数 $\\theta_{\\pi} \\leftarrow  \\theta_{\\pi} - \\alpha_2 \\nabla_{\\pi}\\sum_{i}L^{i}_{Actor}$;   \n",
    "8. 结束。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38725b96",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 算法评价\n",
    "- 优点：可以进行单步更新，不需要跑完一个episode再更新网络参数，相较于传统的PG（Policy Gradient）更新更快。传统PG方法对于价值的估计虽然是无偏估计，但方差较大，AC方法允许少量偏差，但是能够有效降低方差。\n",
    "- 缺点：Actor的行为取决于Critic的Value，但是因为Critic本身就很难收敛，与Actor结合后收敛难度会更大。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2dd4446",
   "metadata": {},
   "source": [
    "## 2.4 gym平台介绍\n",
    "本实验借助了gym平台的环境，该平台由 openai 公司开发，且提供了一整套与平台中虚拟环境进行交互的 api接口，gym 的推出为强化学习算法的研究提供了更好地基准测试平台，同时将各类 环境标准化，使研究员可以专注于算法研究而无需花过多的时间在环境的模拟上。gym 提供一个 step 函数供智能体与环境进行交互，其参数为动作，主要返回值及含义分别为：  \n",
    "- observation：表示智能体所处环境的当前状态，代表着智能体的观察值，例如棋盘上棋子的状态。\n",
    "- reward：表示智能体采取操作后从环境中获得的奖励，其类型可能是整数、小数等，但是具体的规模和类型与具体的规模和类型与环境有关，但是智能体的总目标仍然是获取最大的奖励值。\n",
    "- done: 大多数任务都属于阶段性任务，当到达一定条件的时候表示任务已经结束，比如五子棋游戏中的一方五子相连，机器人在路面上摔倒，或者在规定的步数以内没有完成任务，则都代表任务结束。所以 done 是一个判断条件，类型为布尔值，代表当前任务是否结束，如果结束则可以选择使用 reset 函数重置当前任务。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ed9536e",
   "metadata": {},
   "source": [
    "# 3. 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=2.0；Python环境=3.7\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU CUDA 10.1|Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.0 Python3.7.5 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "847feec8",
   "metadata": {},
   "source": [
    "# 4 数据处理\n",
    "本实验采用Open Gym中的CartPole-v1环境，实际参考了论文：[Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem](https://ieeexplore.ieee.org/document/6313077).中的倒立摆控制问题。  \n",
    "\n",
    "\n",
    "## 4.1 数据准备  \n",
    "  \n",
    "CartPole即车杆游戏，游戏模型如下图所示。游戏里面有一个小车，上面竖着一根杆子，每次重置后的初始状态会有所不同。小车需要左右移动来保持杆子竖直，为了保证游戏继续运行需要满足以下两个条件：    \n",
    "  \n",
    "<img src=\"./Figures/fig_002.png\" width=\"40%\">    \n",
    "  \n",
    "- 杆子的倾斜角 $\\theta$ 不能大于15°;  \n",
    "- 小车的移动范围 $x$ 需要保持在一定范围（中间到两边各2.4个长度单位）;\n",
    "  \n",
    "动作（action）：\n",
    "- 左移（0）;\n",
    "- 右移（1）;  \n",
    "  \n",
    "状态变量（state variables）:\n",
    "- x: 小车在轨道上的位置（position of the cart on the track）;\n",
    "- $\\theta$: 杆子与竖直方向的夹角（angle of the pole with the vertical）;\n",
    "- $\\dot{x}$: 小车速度（cart velocity）;\n",
    "- $\\dot{\\theta}$: 角度变化率（rate of change of the angle）;\n",
    "\n",
    "## 4.2 数据加载"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05493fad",
   "metadata": {},
   "source": [
    "通过导入gym库加载CartPole-v1游戏场景，在gym的CartPole环境中，小车执行左移或右移的action后，env会返回一个+1的reward。本实验使用的CartPole-v1在达到500个reward后，游戏会结束。加载车杆游戏代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a446112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym强化学习库, gym版本为0.21.0\n",
    "import gym\n",
    "# 加载车杆游戏场景\n",
    "env=gym.make('CartPole-v1')\n",
    "# 获取状态数\n",
    "state_number=env.observation_space.shape[0]\n",
    "# 获取动作数\n",
    "action_number=env.action_space.n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4639eaa",
   "metadata": {},
   "source": [
    "# 5.算法实现\n",
    "使用MindSpore实现AC算法并进行训练。\n",
    "## 5.1 导入Python库并配置运行信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bdcf2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os库\n",
    "import os\n",
    "# 引入MindSpore\n",
    "import mindspore\n",
    "# 引入神经网络模块\n",
    "import mindspore.nn as nn\n",
    "# 常见算子操作\n",
    "import mindspore.ops as F\n",
    "# 引入numpy\n",
    "import numpy as np\n",
    "# 引入张量模块\n",
    "from mindspore import Tensor\n",
    "# 配置静态库\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f92fef3",
   "metadata": {},
   "source": [
    "## 5.2 定义参数和超参数\n",
    "定义训练回合数、Actor学习率、Critic学习率以及折现因子 $\\gamma$，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1734276",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_num = 10     # 训练回合数 \n",
    "LR_A = 0.005        # learning rate for actor\n",
    "LR_C = 0.01         # learning rate for critic\n",
    "Gamma = 0.9         # 折现因子"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43b61bc",
   "metadata": {},
   "source": [
    "## 5.3 定义损失函数\n",
    "继承自nn.LossBase类构造损失函数，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e396b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数MAELoss\n",
    "class MAELoss(nn.LossBase):\n",
    "    def __init__(self, reduction=\"none\"):\n",
    "        super(MAELoss, self).__init__(reduction)\n",
    "    # 构造损失函数\n",
    "    def construct(self, prob, a,td):\n",
    "        a_constant=a\n",
    "        log_prob = F.log(prob)\n",
    "        td_constant=td[0]\n",
    "        # 定义信息熵\n",
    "        log_prob_constant=-log_prob[0][a_constant]\n",
    "        # 计算演员损失函数\n",
    "        actor_loss = -log_prob_constant * td_constant\n",
    "        return actor_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7d2385",
   "metadata": {},
   "source": [
    "## 5.4 构建Actor网络"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23aeff43",
   "metadata": {},
   "source": [
    "构建Actor网络类以及Actor类，构建Actor的行为选择函数和学习函数，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9323d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor网络类\n",
    "class ActorNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        # 全连接层1\n",
    "        self.fc1 = nn.Dense(state_number, 50)\n",
    "        # 全连接层2\n",
    "        self.fc2 = nn.Dense(50, 20)\n",
    "        # 全连接层3\n",
    "        self.fc3 = nn.Dense(20, action_number)\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # Sigmoid激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Softmax归一化函数\n",
    "        self.softmax = nn.Softmax()\n",
    "    # 构造Actor网络\n",
    "    def construct(self, x):\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # ReLU激活函数\n",
    "        x = self.relu(x)\n",
    "        # 全连接层\n",
    "        x = self.fc2(x)\n",
    "        # Sigmoid激活函数\n",
    "        x = self.sigmoid(x)\n",
    "        # 全连接层\n",
    "        x = self.fc3(x)\n",
    "        # 返回softmax函数结果\n",
    "        return self.softmax(x)\n",
    "# 演员类\n",
    "class Actor():\n",
    "    def __init__(self):\n",
    "        # 构造演员网络\n",
    "        self.actor=ActorNet()\n",
    "        # 优化器\n",
    "        self.optimizer = nn.Adam(self.actor.trainable_params(),learning_rate=LR_A)\n",
    "        # 损失函数\n",
    "        self.loss=MAELoss()\n",
    "    # 行为选择函数\n",
    "    def choose(self,inputstate):\n",
    "        # inputstate=Tensor(inputstate)\n",
    "        # 输入状态\n",
    "        inputstate=Tensor([inputstate])\n",
    "        probs=self.actor(inputstate).asnumpy()\n",
    "        # 获取行为\n",
    "        action=np.random.choice(np.arange(action_number),p=probs[0])\n",
    "        # 返回行为\n",
    "        return action\n",
    "    # 学习函数\n",
    "    def learn(self,s,a,td):\n",
    "        s = Tensor([s])\n",
    "        prob = self.actor(s)\n",
    "        # log_prob = F.log(prob)\n",
    "        # td张量化\n",
    "        td=Tensor(td)\n",
    "        # a转为整型变量\n",
    "        a=int(a)\n",
    "        a_constant = a\n",
    "        # 信息熵\n",
    "        log_prob = F.log(prob)\n",
    "        td_constant = td[0]\n",
    "        log_prob_constant = -log_prob[0][a_constant]\n",
    "        # 构造损失网络\n",
    "        loss_net=nn.WithLossCell(self.actor,loss_fn=self.loss(prob,Tensor(a),td))\n",
    "        # 构造训练网络\n",
    "        train_net = nn.TrainOneStepCell(loss_net,self.optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87657a5f",
   "metadata": {},
   "source": [
    "## 5.5 构建Critic网络\n",
    "构建Critic网络类以及Critic类，构建Critic的学习函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32eede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评论者网络\n",
    "class CriticNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(CriticNet, self).__init__()\n",
    "        # 全连接层1\n",
    "        self.fc1 = nn.Dense(state_number, 50)\n",
    "        # 全连接层2\n",
    "        self.fc2 = nn.Dense(50, 20)\n",
    "        # 全连接层3\n",
    "        self.fc3 = nn.Dense(20, 1)\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # Sigmoid激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Softmax归一化函数\n",
    "        self.softmax = nn.Softmax()\n",
    "    # 构造函数\n",
    "    def construct(self, x):\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # ReLU激活函数\n",
    "        x = self.relu(x)\n",
    "        # 全连接层\n",
    "        x = self.fc2(x)\n",
    "        # Sigmoid激活函数\n",
    "        x = self.sigmoid(x)\n",
    "        # 返回全连接层结果\n",
    "        return self.fc3(x)\n",
    "class Critic():\n",
    "    def __init__(self):\n",
    "        self.critic=CriticNet()\n",
    "        self.tempA=ActorNet()\n",
    "        # 优化器\n",
    "        self.optimizer = nn.Adam(self.critic.trainable_params(),learning_rate=LR_C)\n",
    "        # 均方误差（MSE）\n",
    "        self.lossfunc=nn.MSELoss()\n",
    "    def learn(self,s,r,s_):\n",
    "        s = Tensor([s])\n",
    "        # 输入当前状态，由网络得到估计v\n",
    "        v=self.critic(s)\n",
    "        r=Tensor([r])\n",
    "        s_ = Tensor([s_])\n",
    "        temp=Tensor(self.critic(s_).asnumpy())\n",
    "        # 真实v\n",
    "        reality_v=r+Gamma*temp[0]\n",
    "        # 构造损失网络\n",
    "        loss_net=nn.WithLossCell(self.critic,self.lossfunc(reality_v,v[0]))\n",
    "        # 构造训练网络\n",
    "        train_net=nn.TrainOneStepCell(loss_net,self.optimizer)\n",
    "        # 计算真实v与估计v之间的差距\n",
    "        advantage=(reality_v-v).asnumpy()\n",
    "        return advantage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15dc302f",
   "metadata": {},
   "source": [
    "# 6 模型训练\n",
    "首先实例化演员类和评论者类，然后开始训练。每轮训练迭代过程中需要重置环境，打印出每个行为所得到的奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05cb1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AC训练中...\n",
      "回合数：0 奖励：25.0 [ 0.1994569  1.3871958 -0.2384345 -2.2478905]\n",
      "回合数：1 奖励：21.0 [-0.08443348 -0.6464958   0.21412012  1.3251446 ]\n",
      "回合数：2 奖励：26.0 [0.08182683 0.00246794 0.22186813 1.0829068 ]\n",
      "回合数：3 奖励：31.0 [-0.2996995  -1.3758174   0.23821151  1.8274974 ]\n",
      "回合数：4 奖励：16.0 [-0.09228256 -0.46052352  0.23157735  1.0725884 ]\n",
      "回合数：5 奖励：27.0 [-0.19192666 -1.0030342   0.22243287  1.8467003 ]\n",
      "回合数：6 奖励：16.0 [-0.14526904 -0.804235    0.23027563  1.6596519 ]\n",
      "回合数：7 奖励：19.0 [-0.08817563 -0.18751512  0.21364075  0.85509807]\n",
      "回合数：8 奖励：25.0 [ 0.11210383  0.98842704 -0.2136335  -1.9466423 ]\n",
      "回合数：9 奖励：81.0 [ 0.5520829   0.5458491  -0.22831152 -0.9592897 ]\n",
      "AC训练完成\n"
     ]
    }
   ],
   "source": [
    "print('AC训练中...')\n",
    "# 演员实例化\n",
    "a=Actor()\n",
    "# 评论者实例化\n",
    "c=Critic()\n",
    "# 开始训练\n",
    "for i in range(round_num):\n",
    "    r_totle=[]\n",
    "    # 环境重置\n",
    "    observation = env.reset()#环境重置\n",
    "    # 判断数据类型（gym版本）\n",
    "    observation = observation if type(observation)== np.ndarray else observation[0]\n",
    "    # print(observation)\n",
    "    # print(observation)\n",
    "    while True:\n",
    "        # 选择行为\n",
    "        action=a.choose(observation)\n",
    "        # print(env.step(action))\n",
    "        step = env.step(action)\n",
    "        # 获取训练过程相关参数\n",
    "        observation_ = step[0]\n",
    "        reward = step[1]\n",
    "        done = step[2]\n",
    "        info = step[3]\n",
    "        # 打印出训练过程\n",
    "        # print(observation_, reward, done, info)\n",
    "        # observation_, reward, done, info = env.step(action)\n",
    "        # 获取td误差\n",
    "        td_error =c.learn(observation,reward,observation_)\n",
    "        # 学习行为\n",
    "        a.learn(observation,action,td_error)\n",
    "        observation=observation_\n",
    "        # 行为选择奖励加入总回报\n",
    "        r_totle.append(reward)\n",
    "        # done = True进行下一轮训练\n",
    "        if done:\n",
    "            break\n",
    "    # 计算总回报\n",
    "    r_sum=sum(r_totle)\n",
    "    # 打印训练回合数和奖励\n",
    "    print(\"\\r回合数：{} 奖励：{}\".format(i,r_sum),end=\" \")\n",
    "    print(observation)\n",
    "    # 保存检查点\n",
    "    if i%100==0 and i > int(round_num/2):\n",
    "        mindspore.save_checkpoint(a.actor, \"./actor.ckpt\")\n",
    "        mindspore.save_checkpoint(c.critic, \"./critic.ckpt\")\n",
    "print('AC训练完成')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('ms_n')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ab65fdc43de4932de46c69418c64f6e3769852db39afdc53804a8e5b8ff4a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
